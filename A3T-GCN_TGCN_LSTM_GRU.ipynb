{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangshuo98767/AI/blob/main/A3T-GCN_TGCN_LSTM_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5I1ZeoGyGB4J",
        "outputId": "30003cb8-b93b-40a5-d64d-d6bad2440f67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-.html\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Requirement already satisfied: torch-geometric-temporal in /usr/local/lib/python3.11/dist-packages (0.54.0)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (2.5.1+cpu)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (3.0.12)\n",
            "Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (1.3.5)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (0.6.18)\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (2.1.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (2.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (3.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.11/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal) (2025.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torch-geometric-temporal) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric->torch-geometric-temporal) (3.11.13)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric->torch-geometric-temporal) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric->torch-geometric-temporal) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric->torch-geometric-temporal) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric->torch-geometric-temporal) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse->torch-geometric-temporal) (1.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torch-geometric-temporal) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2025.1.31)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "#install pytorch and all related libraries\n",
        "#it will take some time\n",
        "\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-${pt_version}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-${pt_version}.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-${pt_version}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${pt_version}.html\n",
        "!pip install torch-geometric\n",
        "!pip install torch-geometric-temporal\n",
        "!pip install keras\n",
        "!pip install --upgrade torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zHeUOEsQuUoh"
      },
      "outputs": [],
      "source": [
        "# import all required libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow import keras\n",
        "from keras.layers import LSTM, GRU, Dense\n",
        "from keras.models import Sequential\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from torch_geometric_temporal.nn.recurrent import A3TGCN, TGCN2\n",
        "from torch_geometric_temporal.signal import StaticGraphTemporalSignalBatch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import mean_squared_error,  mean_absolute_error\n",
        "import scipy.stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "PCbQqRRgDLtN",
        "outputId": "7e932964-2397-4b4f-9581-8876e3dfe840"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          5         47        72        78        88        92        113  \\\n",
              "5    0.000000  0.110356  0.236273  0.143935  0.181212  0.164228  0.112551   \n",
              "47   0.110356  0.000000  0.109257  0.267139  0.095718  0.144390  0.208583   \n",
              "72   0.236273  0.109257  0.000000  0.174832  0.645102  0.325035  0.148464   \n",
              "78   0.143935  0.267139  0.174832  0.000000  0.145813  0.313177  0.442175   \n",
              "88   0.181212  0.095718  0.645102  0.145813  0.000000  0.260419  0.133522   \n",
              "92   0.164228  0.144390  0.325035  0.313177  0.260419  0.000000  0.271064   \n",
              "113  0.112551  0.208583  0.148464  0.442175  0.133522  0.271064  0.000000   \n",
              "126  0.126298  0.132625  0.222798  0.254499  0.211733  0.540075  0.308841   \n",
              "138  0.117328  0.079494  0.221339  0.113106  0.330050  0.176855  0.114254   \n",
              "141  0.117353  0.097987  0.229932  0.153721  0.275207  0.278847  0.168040   \n",
              "142  0.124251  0.107809  0.248793  0.179284  0.274416  0.367075  0.197347   \n",
              "143  0.115321  0.124468  0.196040  0.220380  0.194237  0.379903  0.286935   \n",
              "177  0.095410  0.100376  0.152651  0.150958  0.163359  0.215033  0.191733   \n",
              "181  0.082770  0.128353  0.109845  0.170751  0.107200  0.162230  0.277714   \n",
              "192  0.091417  0.087746  0.147244  0.126098  0.166278  0.180396  0.149161   \n",
              "195  0.083470  0.096880  0.122364  0.135770  0.128027  0.167196  0.180343   \n",
              "217  0.066397  0.102601  0.082703  0.116351  0.081739  0.109029  0.157361   \n",
              "220  0.058654  0.097302  0.068796  0.097973  0.067254  0.086970  0.122457   \n",
              "228  0.075098  0.082217  0.107523  0.109166  0.115331  0.133413  0.135411   \n",
              "243  0.068696  0.069394  0.096458  0.089090  0.106098  0.109384  0.103990   \n",
              "254  0.054583  0.084349  0.064189  0.086394  0.063360  0.079381  0.105806   \n",
              "281  0.059056  0.071076  0.076388  0.084737  0.079434  0.091725  0.103478   \n",
              "296  0.058305  0.064031  0.076561  0.077548  0.081462  0.087882  0.091317   \n",
              "323  0.050814  0.047170  0.064459  0.056153  0.070607  0.065522  0.060882   \n",
              "\n",
              "          126       138       141  ...       192       195       217  \\\n",
              "5    0.126298  0.117328  0.117353  ...  0.091417  0.083470  0.066397   \n",
              "47   0.132625  0.079494  0.097987  ...  0.087746  0.096880  0.102601   \n",
              "72   0.222798  0.221339  0.229932  ...  0.147244  0.122364  0.082703   \n",
              "78   0.254499  0.113106  0.153721  ...  0.126098  0.135770  0.116351   \n",
              "88   0.211733  0.330050  0.275207  ...  0.166278  0.128027  0.081739   \n",
              "92   0.540075  0.176855  0.278847  ...  0.180396  0.167196  0.109029   \n",
              "113  0.308841  0.114254  0.168040  ...  0.149161  0.180343  0.157361   \n",
              "126  0.000000  0.180816  0.365729  ...  0.247734  0.241840  0.131396   \n",
              "138  0.180816  0.000000  0.331141  ...  0.223056  0.144352  0.083149   \n",
              "141  0.365729  0.331141  0.000000  ...  0.409101  0.234668  0.109792   \n",
              "142  0.545834  0.266708  1.075541  ...  0.339723  0.238133  0.115072   \n",
              "143  1.266551  0.181021  0.391533  ...  0.294498  0.298593  0.141066   \n",
              "177  0.344411  0.187088  0.389015  ...  0.643446  0.589755  0.149415   \n",
              "181  0.215382  0.105748  0.155032  ...  0.172656  0.287938  0.332458   \n",
              "192  0.247734  0.223056  0.409101  ...  0.000000  0.383024  0.126843   \n",
              "195  0.241840  0.144352  0.234668  ...  0.383024  0.000000  0.189541   \n",
              "217  0.131396  0.083149  0.109792  ...  0.126843  0.189541  0.000000   \n",
              "220  0.098479  0.066961  0.083599  ...  0.091766  0.120511  0.328968   \n",
              "228  0.175335  0.139938  0.198333  ...  0.368000  0.543167  0.162218   \n",
              "243  0.133346  0.138718  0.165686  ...  0.274047  0.240868  0.120425   \n",
              "254  0.089911  0.064239  0.078932  ...  0.088265  0.114620  0.280409   \n",
              "281  0.110489  0.090053  0.111642  ...  0.149370  0.201828  0.189488   \n",
              "296  0.104375  0.097515  0.114614  ...  0.159146  0.180192  0.130992   \n",
              "323  0.071900  0.089265  0.085327  ...  0.101223  0.090280  0.067329   \n",
              "\n",
              "          220       228       243       254       281       296       323  \n",
              "5    0.058654  0.075098  0.068696  0.054583  0.059056  0.058305  0.050814  \n",
              "47   0.097302  0.082217  0.069394  0.084349  0.071076  0.064031  0.047170  \n",
              "72   0.068796  0.107523  0.096458  0.064189  0.076388  0.076561  0.064459  \n",
              "78   0.097973  0.109166  0.089090  0.086394  0.084737  0.077548  0.056153  \n",
              "88   0.067254  0.115331  0.106098  0.063360  0.079434  0.081462  0.070607  \n",
              "92   0.086970  0.133413  0.109384  0.079381  0.091725  0.087882  0.065522  \n",
              "113  0.122457  0.135411  0.103990  0.105806  0.103478  0.091317  0.060882  \n",
              "126  0.098479  0.175335  0.133346  0.089911  0.110489  0.104375  0.071900  \n",
              "138  0.066961  0.139938  0.138718  0.064239  0.090053  0.097515  0.089265  \n",
              "141  0.083599  0.198333  0.165686  0.078932  0.111642  0.114614  0.085327  \n",
              "142  0.087317  0.188930  0.151688  0.081598  0.110142  0.109845  0.079752  \n",
              "143  0.102567  0.203267  0.147834  0.094033  0.120828  0.113710  0.075350  \n",
              "177  0.103698  0.350084  0.216424  0.097819  0.152886  0.148460  0.088951  \n",
              "181  0.179845  0.197353  0.134133  0.154318  0.159274  0.125521  0.069535  \n",
              "192  0.091766  0.368000  0.274047  0.088265  0.149370  0.159146  0.101223  \n",
              "195  0.120511  0.543167  0.240868  0.114620  0.201828  0.180192  0.090280  \n",
              "217  0.328968  0.162218  0.120425  0.280409  0.189488  0.130992  0.067329  \n",
              "220  0.000000  0.108908  0.088921  0.618468  0.130787  0.098220  0.056897  \n",
              "228  0.108908  0.000000  0.418550  0.107466  0.251363  0.257507  0.107236  \n",
              "243  0.088921  0.418550  0.000000  0.089666  0.213903  0.323453  0.144101  \n",
              "254  0.618468  0.107466  0.089666  0.000000  0.141058  0.103476  0.058424  \n",
              "281  0.130787  0.251363  0.213903  0.141058  0.000000  0.388382  0.099729  \n",
              "296  0.098220  0.257507  0.323453  0.103476  0.388382  0.000000  0.134183  \n",
              "323  0.056897  0.107236  0.144101  0.058424  0.099729  0.134183  0.000000  \n",
              "\n",
              "[24 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea5741ab-0154-4052-a426-d3dd85bf900c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>5</th>\n",
              "      <th>47</th>\n",
              "      <th>72</th>\n",
              "      <th>78</th>\n",
              "      <th>88</th>\n",
              "      <th>92</th>\n",
              "      <th>113</th>\n",
              "      <th>126</th>\n",
              "      <th>138</th>\n",
              "      <th>141</th>\n",
              "      <th>...</th>\n",
              "      <th>192</th>\n",
              "      <th>195</th>\n",
              "      <th>217</th>\n",
              "      <th>220</th>\n",
              "      <th>228</th>\n",
              "      <th>243</th>\n",
              "      <th>254</th>\n",
              "      <th>281</th>\n",
              "      <th>296</th>\n",
              "      <th>323</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110356</td>\n",
              "      <td>0.236273</td>\n",
              "      <td>0.143935</td>\n",
              "      <td>0.181212</td>\n",
              "      <td>0.164228</td>\n",
              "      <td>0.112551</td>\n",
              "      <td>0.126298</td>\n",
              "      <td>0.117328</td>\n",
              "      <td>0.117353</td>\n",
              "      <td>...</td>\n",
              "      <td>0.091417</td>\n",
              "      <td>0.083470</td>\n",
              "      <td>0.066397</td>\n",
              "      <td>0.058654</td>\n",
              "      <td>0.075098</td>\n",
              "      <td>0.068696</td>\n",
              "      <td>0.054583</td>\n",
              "      <td>0.059056</td>\n",
              "      <td>0.058305</td>\n",
              "      <td>0.050814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.110356</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109257</td>\n",
              "      <td>0.267139</td>\n",
              "      <td>0.095718</td>\n",
              "      <td>0.144390</td>\n",
              "      <td>0.208583</td>\n",
              "      <td>0.132625</td>\n",
              "      <td>0.079494</td>\n",
              "      <td>0.097987</td>\n",
              "      <td>...</td>\n",
              "      <td>0.087746</td>\n",
              "      <td>0.096880</td>\n",
              "      <td>0.102601</td>\n",
              "      <td>0.097302</td>\n",
              "      <td>0.082217</td>\n",
              "      <td>0.069394</td>\n",
              "      <td>0.084349</td>\n",
              "      <td>0.071076</td>\n",
              "      <td>0.064031</td>\n",
              "      <td>0.047170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.236273</td>\n",
              "      <td>0.109257</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.174832</td>\n",
              "      <td>0.645102</td>\n",
              "      <td>0.325035</td>\n",
              "      <td>0.148464</td>\n",
              "      <td>0.222798</td>\n",
              "      <td>0.221339</td>\n",
              "      <td>0.229932</td>\n",
              "      <td>...</td>\n",
              "      <td>0.147244</td>\n",
              "      <td>0.122364</td>\n",
              "      <td>0.082703</td>\n",
              "      <td>0.068796</td>\n",
              "      <td>0.107523</td>\n",
              "      <td>0.096458</td>\n",
              "      <td>0.064189</td>\n",
              "      <td>0.076388</td>\n",
              "      <td>0.076561</td>\n",
              "      <td>0.064459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.143935</td>\n",
              "      <td>0.267139</td>\n",
              "      <td>0.174832</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.145813</td>\n",
              "      <td>0.313177</td>\n",
              "      <td>0.442175</td>\n",
              "      <td>0.254499</td>\n",
              "      <td>0.113106</td>\n",
              "      <td>0.153721</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126098</td>\n",
              "      <td>0.135770</td>\n",
              "      <td>0.116351</td>\n",
              "      <td>0.097973</td>\n",
              "      <td>0.109166</td>\n",
              "      <td>0.089090</td>\n",
              "      <td>0.086394</td>\n",
              "      <td>0.084737</td>\n",
              "      <td>0.077548</td>\n",
              "      <td>0.056153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.181212</td>\n",
              "      <td>0.095718</td>\n",
              "      <td>0.645102</td>\n",
              "      <td>0.145813</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.260419</td>\n",
              "      <td>0.133522</td>\n",
              "      <td>0.211733</td>\n",
              "      <td>0.330050</td>\n",
              "      <td>0.275207</td>\n",
              "      <td>...</td>\n",
              "      <td>0.166278</td>\n",
              "      <td>0.128027</td>\n",
              "      <td>0.081739</td>\n",
              "      <td>0.067254</td>\n",
              "      <td>0.115331</td>\n",
              "      <td>0.106098</td>\n",
              "      <td>0.063360</td>\n",
              "      <td>0.079434</td>\n",
              "      <td>0.081462</td>\n",
              "      <td>0.070607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.164228</td>\n",
              "      <td>0.144390</td>\n",
              "      <td>0.325035</td>\n",
              "      <td>0.313177</td>\n",
              "      <td>0.260419</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.271064</td>\n",
              "      <td>0.540075</td>\n",
              "      <td>0.176855</td>\n",
              "      <td>0.278847</td>\n",
              "      <td>...</td>\n",
              "      <td>0.180396</td>\n",
              "      <td>0.167196</td>\n",
              "      <td>0.109029</td>\n",
              "      <td>0.086970</td>\n",
              "      <td>0.133413</td>\n",
              "      <td>0.109384</td>\n",
              "      <td>0.079381</td>\n",
              "      <td>0.091725</td>\n",
              "      <td>0.087882</td>\n",
              "      <td>0.065522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>0.112551</td>\n",
              "      <td>0.208583</td>\n",
              "      <td>0.148464</td>\n",
              "      <td>0.442175</td>\n",
              "      <td>0.133522</td>\n",
              "      <td>0.271064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.308841</td>\n",
              "      <td>0.114254</td>\n",
              "      <td>0.168040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.149161</td>\n",
              "      <td>0.180343</td>\n",
              "      <td>0.157361</td>\n",
              "      <td>0.122457</td>\n",
              "      <td>0.135411</td>\n",
              "      <td>0.103990</td>\n",
              "      <td>0.105806</td>\n",
              "      <td>0.103478</td>\n",
              "      <td>0.091317</td>\n",
              "      <td>0.060882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>0.126298</td>\n",
              "      <td>0.132625</td>\n",
              "      <td>0.222798</td>\n",
              "      <td>0.254499</td>\n",
              "      <td>0.211733</td>\n",
              "      <td>0.540075</td>\n",
              "      <td>0.308841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.180816</td>\n",
              "      <td>0.365729</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247734</td>\n",
              "      <td>0.241840</td>\n",
              "      <td>0.131396</td>\n",
              "      <td>0.098479</td>\n",
              "      <td>0.175335</td>\n",
              "      <td>0.133346</td>\n",
              "      <td>0.089911</td>\n",
              "      <td>0.110489</td>\n",
              "      <td>0.104375</td>\n",
              "      <td>0.071900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>0.117328</td>\n",
              "      <td>0.079494</td>\n",
              "      <td>0.221339</td>\n",
              "      <td>0.113106</td>\n",
              "      <td>0.330050</td>\n",
              "      <td>0.176855</td>\n",
              "      <td>0.114254</td>\n",
              "      <td>0.180816</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.331141</td>\n",
              "      <td>...</td>\n",
              "      <td>0.223056</td>\n",
              "      <td>0.144352</td>\n",
              "      <td>0.083149</td>\n",
              "      <td>0.066961</td>\n",
              "      <td>0.139938</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.064239</td>\n",
              "      <td>0.090053</td>\n",
              "      <td>0.097515</td>\n",
              "      <td>0.089265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>0.117353</td>\n",
              "      <td>0.097987</td>\n",
              "      <td>0.229932</td>\n",
              "      <td>0.153721</td>\n",
              "      <td>0.275207</td>\n",
              "      <td>0.278847</td>\n",
              "      <td>0.168040</td>\n",
              "      <td>0.365729</td>\n",
              "      <td>0.331141</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.409101</td>\n",
              "      <td>0.234668</td>\n",
              "      <td>0.109792</td>\n",
              "      <td>0.083599</td>\n",
              "      <td>0.198333</td>\n",
              "      <td>0.165686</td>\n",
              "      <td>0.078932</td>\n",
              "      <td>0.111642</td>\n",
              "      <td>0.114614</td>\n",
              "      <td>0.085327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>0.124251</td>\n",
              "      <td>0.107809</td>\n",
              "      <td>0.248793</td>\n",
              "      <td>0.179284</td>\n",
              "      <td>0.274416</td>\n",
              "      <td>0.367075</td>\n",
              "      <td>0.197347</td>\n",
              "      <td>0.545834</td>\n",
              "      <td>0.266708</td>\n",
              "      <td>1.075541</td>\n",
              "      <td>...</td>\n",
              "      <td>0.339723</td>\n",
              "      <td>0.238133</td>\n",
              "      <td>0.115072</td>\n",
              "      <td>0.087317</td>\n",
              "      <td>0.188930</td>\n",
              "      <td>0.151688</td>\n",
              "      <td>0.081598</td>\n",
              "      <td>0.110142</td>\n",
              "      <td>0.109845</td>\n",
              "      <td>0.079752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>0.115321</td>\n",
              "      <td>0.124468</td>\n",
              "      <td>0.196040</td>\n",
              "      <td>0.220380</td>\n",
              "      <td>0.194237</td>\n",
              "      <td>0.379903</td>\n",
              "      <td>0.286935</td>\n",
              "      <td>1.266551</td>\n",
              "      <td>0.181021</td>\n",
              "      <td>0.391533</td>\n",
              "      <td>...</td>\n",
              "      <td>0.294498</td>\n",
              "      <td>0.298593</td>\n",
              "      <td>0.141066</td>\n",
              "      <td>0.102567</td>\n",
              "      <td>0.203267</td>\n",
              "      <td>0.147834</td>\n",
              "      <td>0.094033</td>\n",
              "      <td>0.120828</td>\n",
              "      <td>0.113710</td>\n",
              "      <td>0.075350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>0.095410</td>\n",
              "      <td>0.100376</td>\n",
              "      <td>0.152651</td>\n",
              "      <td>0.150958</td>\n",
              "      <td>0.163359</td>\n",
              "      <td>0.215033</td>\n",
              "      <td>0.191733</td>\n",
              "      <td>0.344411</td>\n",
              "      <td>0.187088</td>\n",
              "      <td>0.389015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.643446</td>\n",
              "      <td>0.589755</td>\n",
              "      <td>0.149415</td>\n",
              "      <td>0.103698</td>\n",
              "      <td>0.350084</td>\n",
              "      <td>0.216424</td>\n",
              "      <td>0.097819</td>\n",
              "      <td>0.152886</td>\n",
              "      <td>0.148460</td>\n",
              "      <td>0.088951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>0.082770</td>\n",
              "      <td>0.128353</td>\n",
              "      <td>0.109845</td>\n",
              "      <td>0.170751</td>\n",
              "      <td>0.107200</td>\n",
              "      <td>0.162230</td>\n",
              "      <td>0.277714</td>\n",
              "      <td>0.215382</td>\n",
              "      <td>0.105748</td>\n",
              "      <td>0.155032</td>\n",
              "      <td>...</td>\n",
              "      <td>0.172656</td>\n",
              "      <td>0.287938</td>\n",
              "      <td>0.332458</td>\n",
              "      <td>0.179845</td>\n",
              "      <td>0.197353</td>\n",
              "      <td>0.134133</td>\n",
              "      <td>0.154318</td>\n",
              "      <td>0.159274</td>\n",
              "      <td>0.125521</td>\n",
              "      <td>0.069535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>0.091417</td>\n",
              "      <td>0.087746</td>\n",
              "      <td>0.147244</td>\n",
              "      <td>0.126098</td>\n",
              "      <td>0.166278</td>\n",
              "      <td>0.180396</td>\n",
              "      <td>0.149161</td>\n",
              "      <td>0.247734</td>\n",
              "      <td>0.223056</td>\n",
              "      <td>0.409101</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.383024</td>\n",
              "      <td>0.126843</td>\n",
              "      <td>0.091766</td>\n",
              "      <td>0.368000</td>\n",
              "      <td>0.274047</td>\n",
              "      <td>0.088265</td>\n",
              "      <td>0.149370</td>\n",
              "      <td>0.159146</td>\n",
              "      <td>0.101223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>0.083470</td>\n",
              "      <td>0.096880</td>\n",
              "      <td>0.122364</td>\n",
              "      <td>0.135770</td>\n",
              "      <td>0.128027</td>\n",
              "      <td>0.167196</td>\n",
              "      <td>0.180343</td>\n",
              "      <td>0.241840</td>\n",
              "      <td>0.144352</td>\n",
              "      <td>0.234668</td>\n",
              "      <td>...</td>\n",
              "      <td>0.383024</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.189541</td>\n",
              "      <td>0.120511</td>\n",
              "      <td>0.543167</td>\n",
              "      <td>0.240868</td>\n",
              "      <td>0.114620</td>\n",
              "      <td>0.201828</td>\n",
              "      <td>0.180192</td>\n",
              "      <td>0.090280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>0.066397</td>\n",
              "      <td>0.102601</td>\n",
              "      <td>0.082703</td>\n",
              "      <td>0.116351</td>\n",
              "      <td>0.081739</td>\n",
              "      <td>0.109029</td>\n",
              "      <td>0.157361</td>\n",
              "      <td>0.131396</td>\n",
              "      <td>0.083149</td>\n",
              "      <td>0.109792</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126843</td>\n",
              "      <td>0.189541</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.328968</td>\n",
              "      <td>0.162218</td>\n",
              "      <td>0.120425</td>\n",
              "      <td>0.280409</td>\n",
              "      <td>0.189488</td>\n",
              "      <td>0.130992</td>\n",
              "      <td>0.067329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>0.058654</td>\n",
              "      <td>0.097302</td>\n",
              "      <td>0.068796</td>\n",
              "      <td>0.097973</td>\n",
              "      <td>0.067254</td>\n",
              "      <td>0.086970</td>\n",
              "      <td>0.122457</td>\n",
              "      <td>0.098479</td>\n",
              "      <td>0.066961</td>\n",
              "      <td>0.083599</td>\n",
              "      <td>...</td>\n",
              "      <td>0.091766</td>\n",
              "      <td>0.120511</td>\n",
              "      <td>0.328968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.108908</td>\n",
              "      <td>0.088921</td>\n",
              "      <td>0.618468</td>\n",
              "      <td>0.130787</td>\n",
              "      <td>0.098220</td>\n",
              "      <td>0.056897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>0.075098</td>\n",
              "      <td>0.082217</td>\n",
              "      <td>0.107523</td>\n",
              "      <td>0.109166</td>\n",
              "      <td>0.115331</td>\n",
              "      <td>0.133413</td>\n",
              "      <td>0.135411</td>\n",
              "      <td>0.175335</td>\n",
              "      <td>0.139938</td>\n",
              "      <td>0.198333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.368000</td>\n",
              "      <td>0.543167</td>\n",
              "      <td>0.162218</td>\n",
              "      <td>0.108908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.418550</td>\n",
              "      <td>0.107466</td>\n",
              "      <td>0.251363</td>\n",
              "      <td>0.257507</td>\n",
              "      <td>0.107236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>0.068696</td>\n",
              "      <td>0.069394</td>\n",
              "      <td>0.096458</td>\n",
              "      <td>0.089090</td>\n",
              "      <td>0.106098</td>\n",
              "      <td>0.109384</td>\n",
              "      <td>0.103990</td>\n",
              "      <td>0.133346</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.165686</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274047</td>\n",
              "      <td>0.240868</td>\n",
              "      <td>0.120425</td>\n",
              "      <td>0.088921</td>\n",
              "      <td>0.418550</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.089666</td>\n",
              "      <td>0.213903</td>\n",
              "      <td>0.323453</td>\n",
              "      <td>0.144101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>0.054583</td>\n",
              "      <td>0.084349</td>\n",
              "      <td>0.064189</td>\n",
              "      <td>0.086394</td>\n",
              "      <td>0.063360</td>\n",
              "      <td>0.079381</td>\n",
              "      <td>0.105806</td>\n",
              "      <td>0.089911</td>\n",
              "      <td>0.064239</td>\n",
              "      <td>0.078932</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088265</td>\n",
              "      <td>0.114620</td>\n",
              "      <td>0.280409</td>\n",
              "      <td>0.618468</td>\n",
              "      <td>0.107466</td>\n",
              "      <td>0.089666</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.141058</td>\n",
              "      <td>0.103476</td>\n",
              "      <td>0.058424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>0.059056</td>\n",
              "      <td>0.071076</td>\n",
              "      <td>0.076388</td>\n",
              "      <td>0.084737</td>\n",
              "      <td>0.079434</td>\n",
              "      <td>0.091725</td>\n",
              "      <td>0.103478</td>\n",
              "      <td>0.110489</td>\n",
              "      <td>0.090053</td>\n",
              "      <td>0.111642</td>\n",
              "      <td>...</td>\n",
              "      <td>0.149370</td>\n",
              "      <td>0.201828</td>\n",
              "      <td>0.189488</td>\n",
              "      <td>0.130787</td>\n",
              "      <td>0.251363</td>\n",
              "      <td>0.213903</td>\n",
              "      <td>0.141058</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.388382</td>\n",
              "      <td>0.099729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>0.058305</td>\n",
              "      <td>0.064031</td>\n",
              "      <td>0.076561</td>\n",
              "      <td>0.077548</td>\n",
              "      <td>0.081462</td>\n",
              "      <td>0.087882</td>\n",
              "      <td>0.091317</td>\n",
              "      <td>0.104375</td>\n",
              "      <td>0.097515</td>\n",
              "      <td>0.114614</td>\n",
              "      <td>...</td>\n",
              "      <td>0.159146</td>\n",
              "      <td>0.180192</td>\n",
              "      <td>0.130992</td>\n",
              "      <td>0.098220</td>\n",
              "      <td>0.257507</td>\n",
              "      <td>0.323453</td>\n",
              "      <td>0.103476</td>\n",
              "      <td>0.388382</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.134183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>0.050814</td>\n",
              "      <td>0.047170</td>\n",
              "      <td>0.064459</td>\n",
              "      <td>0.056153</td>\n",
              "      <td>0.070607</td>\n",
              "      <td>0.065522</td>\n",
              "      <td>0.060882</td>\n",
              "      <td>0.071900</td>\n",
              "      <td>0.089265</td>\n",
              "      <td>0.085327</td>\n",
              "      <td>...</td>\n",
              "      <td>0.101223</td>\n",
              "      <td>0.090280</td>\n",
              "      <td>0.067329</td>\n",
              "      <td>0.056897</td>\n",
              "      <td>0.107236</td>\n",
              "      <td>0.144101</td>\n",
              "      <td>0.058424</td>\n",
              "      <td>0.099729</td>\n",
              "      <td>0.134183</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24 rows × 24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea5741ab-0154-4052-a426-d3dd85bf900c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ea5741ab-0154-4052-a426-d3dd85bf900c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ea5741ab-0154-4052-a426-d3dd85bf900c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7d5c7dea-df1f-4c2c-92a1-b80a3a3a376d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7d5c7dea-df1f-4c2c-92a1-b80a3a3a376d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7d5c7dea-df1f-4c2c-92a1-b80a3a3a376d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_0fddc1cb-403e-4089-a354-7283bb03bc61\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('adj_mat_complete')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0fddc1cb-403e-4089-a354-7283bb03bc61 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('adj_mat_complete');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "adj_mat_complete"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#Creating adjacency matrix\n",
        "\n",
        "data_adj= pd.read_csv('/content/distanceNodes.txt', encoding='utf8', delimiter='\\t')\n",
        "\n",
        "#create new column calling Distance_KM, which calculates distance between nodes in km\n",
        "data_adj['Distance_KM']=data_adj['NEAR_DIST']/1000\n",
        "\n",
        "#  Change the nodes id\n",
        "replacement_mapping_dict = {\n",
        "    0: 141, 1: 143, 2: 195, 3: 181, 4: 5, 5: 88, 6: 138, 7: 254, 8: 142, 9: 113, 10: 192, 11: 243, 12: 78, 13: 92,\n",
        "    14: 177, 15: 126, 16: 228, 17: 47, 18: 220, 19: 72, 20: 281, 21: 323, 22:217, 23:296\n",
        "}\n",
        "data_fin = data_adj[[\"IN_FID\", \"NEAR_FID\"]].replace(replacement_mapping_dict)\n",
        "\n",
        "# Replace columns with the new created columns\n",
        "data_adj['IN_FID'] = data_fin[\"IN_FID\"]\n",
        "data_adj['NEAR_FID'] = data_fin[\"NEAR_FID\"]\n",
        "\n",
        "### create Adjacency matrix and replace column and index names according to grid cells id where air quality stations are located\n",
        "\n",
        "am = pd.DataFrame(np.zeros(shape=(24, 24)))\n",
        "am.rename(columns=replacement_mapping_dict, index =replacement_mapping_dict, inplace=True)\n",
        "adj_mat = am.sort_index(axis=1)\n",
        "adj_mat_complete = adj_mat.sort_index()\n",
        "\n",
        "\n",
        "### Adjacency matrix\n",
        "\n",
        "for i in data_adj.IN_FID.unique():\n",
        "  for j in data_adj.NEAR_FID.unique():\n",
        "    if i==j:\n",
        "      adj_mat_complete.at[i,j]=0\n",
        "    else:\n",
        "      adj_mat_complete.at[i,j]=1/data_adj.loc[(data_adj['IN_FID'] == i) & (data_adj['NEAR_FID'] == j)]['Distance_KM']\n",
        "\n",
        "\n",
        "adj_mat_complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eObnLILBecy6"
      },
      "outputs": [],
      "source": [
        "# set the seed in order to provide reproducibility of the code\n",
        "\n",
        "rnd_seed = 11\n",
        "\n",
        "def set_seed(seed_num) -> None:\n",
        "  random.seed(seed_num)\n",
        "  np.random.seed(seed_num)\n",
        "  tf.random.set_seed(seed_num)\n",
        "  tf.experimental.numpy.random.seed(seed_num)\n",
        "  tf.random.set_seed(seed_num)\n",
        "  torch.manual_seed(seed_num)\n",
        "  torch.cuda.manual_seed(seed_num)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  # When running on the CuDNN backend, two further options must be set\n",
        "  os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "  # Set a fixed value for the hash seed\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(seed_num)\n",
        "  print(f\"Random seed set as {seed_num}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "F4dl8jx5en9j"
      },
      "outputs": [],
      "source": [
        "# read the file containing nodes (air quality monitoring stations) features as pandas dataframe\n",
        "Mad_data_2019 = pd.read_csv('/content/Mad_Station_2019.csv')\n",
        "Mad_data_2022 = pd.read_csv('/content/Mad_Station_2022.csv')\n",
        "\n",
        "# to delete 'windDir' column as we will not need it for further analysis.\n",
        "Mad_data_2019 = Mad_data_2019.drop(['windDir'], axis=1)\n",
        "Mad_data_2022 = Mad_data_2022.drop(['windDir'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vUWMs4WCl1c",
        "outputId": "5bcb5636-1de0-4538-e870-7716b99297fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2476, 24, 18)\n",
            "(2497, 24, 18)\n"
          ]
        }
      ],
      "source": [
        "# convert dataframes to numpy arrays\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 先将DataFrame转换为一维数组\n",
        "data_arr = Mad_data_2019.to_numpy().flatten()\n",
        "data_arr_test = Mad_data_2022.to_numpy().flatten()\n",
        "\n",
        "# 计算可以整除432的最大长度\n",
        "n_total = data_arr.size // 432 * 432\n",
        "n_total_test = data_arr_test.size // 432 * 432\n",
        "\n",
        "# 截取整除部分\n",
        "data_trimmed = data_arr[:n_total]\n",
        "data_trimmed_test = data_arr_test[:n_total_test]\n",
        "\n",
        "# 重塑为(-1, 24, 18)\n",
        "fin_data = data_trimmed.reshape(-1, 24, 18)\n",
        "fin_data_test = data_trimmed_test.reshape(-1, 24, 18)\n",
        "\n",
        "print(fin_data.shape)\n",
        "print(fin_data_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lv-G6JcqZzPC"
      },
      "outputs": [],
      "source": [
        "# the function to return data in original scale (reversing Z score)\n",
        "\n",
        "def reverse_zscore(pandas_series, mean, std):\n",
        "    '''Mean and standard deviation should be of original variable before standardisation'''\n",
        "    yis=pandas_series*std+mean\n",
        "    return yis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci9Vjwfps_yp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rny1xvRsDWh"
      },
      "source": [
        "Create the models and run them (A3T-GCN, TGCN, LSTM and GRU)\n",
        "\n",
        "A3T-GCN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f_wOrIaC5zFf"
      },
      "outputs": [],
      "source": [
        "#to convert adjacency dataframe to numpy\n",
        "adj = adj_mat_complete.to_numpy()\n",
        "\n",
        "# standardise train data\n",
        "\n",
        "data = fin_data.transpose(\n",
        "            (1, 2, 0)\n",
        "        )\n",
        "data = data.astype(np.float32)\n",
        "\n",
        "# standardise (via Z-Score Method)\n",
        "means = np.mean(data, axis=(0, 2))\n",
        "data_norm= data-means.reshape(1, -1, 1)\n",
        "stds = np.std(data_norm, axis=(0, 2))\n",
        "data_norm= data_norm/ stds.reshape(1, -1, 1)\n",
        "\n",
        "#to convert adjacency matrix and standardised train data to torch\n",
        "adj = torch.from_numpy(adj)\n",
        "data_norm= torch.from_numpy(data_norm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1RWv84L955Cc"
      },
      "outputs": [],
      "source": [
        "# standardise test data using means and standard deviation of train set\n",
        "\n",
        "data_test = fin_data_test.transpose(\n",
        "            (1, 2, 0)\n",
        "        )\n",
        "data_test = data_test.astype(np.float32)\n",
        "data_test_norm= data_test- means.reshape(1, -1, 1)\n",
        "data_test_norm= data_test_norm/ stds.reshape(1, -1, 1)\n",
        "\n",
        "#to convert standardised test data to torch\n",
        "data_test_norm = torch.from_numpy(data_test_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j9q2CEB-Om6",
        "outputId": "25fb1eeb-10da-48ec-a561-87d73b8df1cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([24, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "adj.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdK3LHqP-QaQ",
        "outputId": "38d946d3-b694-4da5-fdd8-6a06afb3aac5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([24, 18, 2497])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "data_test_norm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ENE2S1bjgsYb"
      },
      "outputs": [],
      "source": [
        "#from adjacency matrix extract the edge indices and edge weights\n",
        "\n",
        "edge_indices, values = dense_to_sparse(adj)\n",
        "edge_indices = edge_indices.numpy()\n",
        "values = values.numpy()\n",
        "edges = edge_indices\n",
        "edge_weights = values\n",
        "batch =64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie4hIMlm-pcg",
        "outputId": "cf440315-dbab-476a-c844-22cfa78964c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 552)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "edges.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm83WCRV-rtQ",
        "outputId": "a7527c5f-b5f8-4323-be26-5a3308dd9be5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(552,)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "edge_weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ibGbQjvdS0F6"
      },
      "outputs": [],
      "source": [
        "class MadridDatasetLoader(object):\n",
        "    \"\"\"The dataset is based on 24 stations (nodes) each having 18 features (nodal features)\n",
        "    and 276 edges connecting each pair of nodes, the edge weights are the distance between the edges.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_norm, edges, edge_weights, batch):\n",
        "        super(MadridDatasetLoader, self).__init__()\n",
        "\n",
        "        self.data_norm = data_norm\n",
        "        self.edges = edges\n",
        "        self.edge_weights= edge_weights\n",
        "        self.batch = batch\n",
        "\n",
        "\n",
        "    def _generate_task(self, num_timesteps_in: int = 6, num_timesteps_out: int = 6):\n",
        "        \"\"\"Uses the node features of the graph and generates a feature/target\n",
        "        relationship of the shape\n",
        "        (num_nodes, num_node_features, num_timesteps_in) -> (num_nodes, num_timesteps_out)\n",
        "\n",
        "\n",
        "        Args:\n",
        "            num_timesteps_in (int): number of timesteps the sequence model sees\n",
        "            num_timesteps_out (int): number of timesteps the sequence model has to predict\n",
        "        \"\"\"\n",
        "        time_steps_starter =   0 # it can be assigned as one of the following {0, 12, 24, 36}\n",
        "        indices = [\n",
        "            (i, i +time_steps_starter+ (num_timesteps_in + num_timesteps_out))\n",
        "            for i in range(self.data_norm.shape[2] - (time_steps_starter+num_timesteps_in + num_timesteps_out) + 1)\n",
        "        ]\n",
        "        print(indices)\n",
        "        # Generate observations\n",
        "        features, target = [], []\n",
        "        for i, j in indices:\n",
        "            features.append((self.data_norm[:, :, i : i + num_timesteps_in]).numpy())\n",
        "            target.append((self.data_norm[  :, 0, i + num_timesteps_in +time_steps_starter: j]).numpy())\n",
        "\n",
        "        self.features = features\n",
        "        self.targets = target\n",
        "\n",
        "    def get_dataset(\n",
        "            self, num_timesteps_in: int = 6, num_timesteps_out: int = 6\n",
        "    ) -> StaticGraphTemporalSignalBatch:\n",
        "        \"\"\"Returns data iterator for the dataset as an instance of the\n",
        "        static graph temporal signal class.\n",
        "\n",
        "        Return types:\n",
        "            * **dataset** *(StaticGraphTemporalSignalBatch)* - The forecasting dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        self._generate_task(num_timesteps_in, num_timesteps_out)\n",
        "        dataset = StaticGraphTemporalSignalBatch(\n",
        "            self.edges, self.edge_weights, self.features, self.targets, self.batch\n",
        "        )\n",
        "\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZCa7JCuY-syX",
        "outputId": "3615a79a-2eab-449b-84e5-20fa98bcd565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 24), (1, 25), (2, 26), (3, 27), (4, 28), (5, 29), (6, 30), (7, 31), (8, 32), (9, 33), (10, 34), (11, 35), (12, 36), (13, 37), (14, 38), (15, 39), (16, 40), (17, 41), (18, 42), (19, 43), (20, 44), (21, 45), (22, 46), (23, 47), (24, 48), (25, 49), (26, 50), (27, 51), (28, 52), (29, 53), (30, 54), (31, 55), (32, 56), (33, 57), (34, 58), (35, 59), (36, 60), (37, 61), (38, 62), (39, 63), (40, 64), (41, 65), (42, 66), (43, 67), (44, 68), (45, 69), (46, 70), (47, 71), (48, 72), (49, 73), (50, 74), (51, 75), (52, 76), (53, 77), (54, 78), (55, 79), (56, 80), (57, 81), (58, 82), (59, 83), (60, 84), (61, 85), (62, 86), (63, 87), (64, 88), (65, 89), (66, 90), (67, 91), (68, 92), (69, 93), (70, 94), (71, 95), (72, 96), (73, 97), (74, 98), (75, 99), (76, 100), (77, 101), (78, 102), (79, 103), (80, 104), (81, 105), (82, 106), (83, 107), (84, 108), (85, 109), (86, 110), (87, 111), (88, 112), (89, 113), (90, 114), (91, 115), (92, 116), (93, 117), (94, 118), (95, 119), (96, 120), (97, 121), (98, 122), (99, 123), (100, 124), (101, 125), (102, 126), (103, 127), (104, 128), (105, 129), (106, 130), (107, 131), (108, 132), (109, 133), (110, 134), (111, 135), (112, 136), (113, 137), (114, 138), (115, 139), (116, 140), (117, 141), (118, 142), (119, 143), (120, 144), (121, 145), (122, 146), (123, 147), (124, 148), (125, 149), (126, 150), (127, 151), (128, 152), (129, 153), (130, 154), (131, 155), (132, 156), (133, 157), (134, 158), (135, 159), (136, 160), (137, 161), (138, 162), (139, 163), (140, 164), (141, 165), (142, 166), (143, 167), (144, 168), (145, 169), (146, 170), (147, 171), (148, 172), (149, 173), (150, 174), (151, 175), (152, 176), (153, 177), (154, 178), (155, 179), (156, 180), (157, 181), (158, 182), (159, 183), (160, 184), (161, 185), (162, 186), (163, 187), (164, 188), (165, 189), (166, 190), (167, 191), (168, 192), (169, 193), (170, 194), (171, 195), (172, 196), (173, 197), (174, 198), (175, 199), (176, 200), (177, 201), (178, 202), (179, 203), (180, 204), (181, 205), (182, 206), (183, 207), (184, 208), (185, 209), (186, 210), (187, 211), (188, 212), (189, 213), (190, 214), (191, 215), (192, 216), (193, 217), (194, 218), (195, 219), (196, 220), (197, 221), (198, 222), (199, 223), (200, 224), (201, 225), (202, 226), (203, 227), (204, 228), (205, 229), (206, 230), (207, 231), (208, 232), (209, 233), (210, 234), (211, 235), (212, 236), (213, 237), (214, 238), (215, 239), (216, 240), (217, 241), (218, 242), (219, 243), (220, 244), (221, 245), (222, 246), (223, 247), (224, 248), (225, 249), (226, 250), (227, 251), (228, 252), (229, 253), (230, 254), (231, 255), (232, 256), (233, 257), (234, 258), (235, 259), (236, 260), (237, 261), (238, 262), (239, 263), (240, 264), (241, 265), (242, 266), (243, 267), (244, 268), (245, 269), (246, 270), (247, 271), (248, 272), (249, 273), (250, 274), (251, 275), (252, 276), (253, 277), (254, 278), (255, 279), (256, 280), (257, 281), (258, 282), (259, 283), (260, 284), (261, 285), (262, 286), (263, 287), (264, 288), (265, 289), (266, 290), (267, 291), (268, 292), (269, 293), (270, 294), (271, 295), (272, 296), (273, 297), (274, 298), (275, 299), (276, 300), (277, 301), (278, 302), (279, 303), (280, 304), (281, 305), (282, 306), (283, 307), (284, 308), (285, 309), (286, 310), (287, 311), (288, 312), (289, 313), (290, 314), (291, 315), (292, 316), (293, 317), (294, 318), (295, 319), (296, 320), (297, 321), (298, 322), (299, 323), (300, 324), (301, 325), (302, 326), (303, 327), (304, 328), (305, 329), (306, 330), (307, 331), (308, 332), (309, 333), (310, 334), (311, 335), (312, 336), (313, 337), (314, 338), (315, 339), (316, 340), (317, 341), (318, 342), (319, 343), (320, 344), (321, 345), (322, 346), (323, 347), (324, 348), (325, 349), (326, 350), (327, 351), (328, 352), (329, 353), (330, 354), (331, 355), (332, 356), (333, 357), (334, 358), (335, 359), (336, 360), (337, 361), (338, 362), (339, 363), (340, 364), (341, 365), (342, 366), (343, 367), (344, 368), (345, 369), (346, 370), (347, 371), (348, 372), (349, 373), (350, 374), (351, 375), (352, 376), (353, 377), (354, 378), (355, 379), (356, 380), (357, 381), (358, 382), (359, 383), (360, 384), (361, 385), (362, 386), (363, 387), (364, 388), (365, 389), (366, 390), (367, 391), (368, 392), (369, 393), (370, 394), (371, 395), (372, 396), (373, 397), (374, 398), (375, 399), (376, 400), (377, 401), (378, 402), (379, 403), (380, 404), (381, 405), (382, 406), (383, 407), (384, 408), (385, 409), (386, 410), (387, 411), (388, 412), (389, 413), (390, 414), (391, 415), (392, 416), (393, 417), (394, 418), (395, 419), (396, 420), (397, 421), (398, 422), (399, 423), (400, 424), (401, 425), (402, 426), (403, 427), (404, 428), (405, 429), (406, 430), (407, 431), (408, 432), (409, 433), (410, 434), (411, 435), (412, 436), (413, 437), (414, 438), (415, 439), (416, 440), (417, 441), (418, 442), (419, 443), (420, 444), (421, 445), (422, 446), (423, 447), (424, 448), (425, 449), (426, 450), (427, 451), (428, 452), (429, 453), (430, 454), (431, 455), (432, 456), (433, 457), (434, 458), (435, 459), (436, 460), (437, 461), (438, 462), (439, 463), (440, 464), (441, 465), (442, 466), (443, 467), (444, 468), (445, 469), (446, 470), (447, 471), (448, 472), (449, 473), (450, 474), (451, 475), (452, 476), (453, 477), (454, 478), (455, 479), (456, 480), (457, 481), (458, 482), (459, 483), (460, 484), (461, 485), (462, 486), (463, 487), (464, 488), (465, 489), (466, 490), (467, 491), (468, 492), (469, 493), (470, 494), (471, 495), (472, 496), (473, 497), (474, 498), (475, 499), (476, 500), (477, 501), (478, 502), (479, 503), (480, 504), (481, 505), (482, 506), (483, 507), (484, 508), (485, 509), (486, 510), (487, 511), (488, 512), (489, 513), (490, 514), (491, 515), (492, 516), (493, 517), (494, 518), (495, 519), (496, 520), (497, 521), (498, 522), (499, 523), (500, 524), (501, 525), (502, 526), (503, 527), (504, 528), (505, 529), (506, 530), (507, 531), (508, 532), (509, 533), (510, 534), (511, 535), (512, 536), (513, 537), (514, 538), (515, 539), (516, 540), (517, 541), (518, 542), (519, 543), (520, 544), (521, 545), (522, 546), (523, 547), (524, 548), (525, 549), (526, 550), (527, 551), (528, 552), (529, 553), (530, 554), (531, 555), (532, 556), (533, 557), (534, 558), (535, 559), (536, 560), (537, 561), (538, 562), (539, 563), (540, 564), (541, 565), (542, 566), (543, 567), (544, 568), (545, 569), (546, 570), (547, 571), (548, 572), (549, 573), (550, 574), (551, 575), (552, 576), (553, 577), (554, 578), (555, 579), (556, 580), (557, 581), (558, 582), (559, 583), (560, 584), (561, 585), (562, 586), (563, 587), (564, 588), (565, 589), (566, 590), (567, 591), (568, 592), (569, 593), (570, 594), (571, 595), (572, 596), (573, 597), (574, 598), (575, 599), (576, 600), (577, 601), (578, 602), (579, 603), (580, 604), (581, 605), (582, 606), (583, 607), (584, 608), (585, 609), (586, 610), (587, 611), (588, 612), (589, 613), (590, 614), (591, 615), (592, 616), (593, 617), (594, 618), (595, 619), (596, 620), (597, 621), (598, 622), (599, 623), (600, 624), (601, 625), (602, 626), (603, 627), (604, 628), (605, 629), (606, 630), (607, 631), (608, 632), (609, 633), (610, 634), (611, 635), (612, 636), (613, 637), (614, 638), (615, 639), (616, 640), (617, 641), (618, 642), (619, 643), (620, 644), (621, 645), (622, 646), (623, 647), (624, 648), (625, 649), (626, 650), (627, 651), (628, 652), (629, 653), (630, 654), (631, 655), (632, 656), (633, 657), (634, 658), (635, 659), (636, 660), (637, 661), (638, 662), (639, 663), (640, 664), (641, 665), (642, 666), (643, 667), (644, 668), (645, 669), (646, 670), (647, 671), (648, 672), (649, 673), (650, 674), (651, 675), (652, 676), (653, 677), (654, 678), (655, 679), (656, 680), (657, 681), (658, 682), (659, 683), (660, 684), (661, 685), (662, 686), (663, 687), (664, 688), (665, 689), (666, 690), (667, 691), (668, 692), (669, 693), (670, 694), (671, 695), (672, 696), (673, 697), (674, 698), (675, 699), (676, 700), (677, 701), (678, 702), (679, 703), (680, 704), (681, 705), (682, 706), (683, 707), (684, 708), (685, 709), (686, 710), (687, 711), (688, 712), (689, 713), (690, 714), (691, 715), (692, 716), (693, 717), (694, 718), (695, 719), (696, 720), (697, 721), (698, 722), (699, 723), (700, 724), (701, 725), (702, 726), (703, 727), (704, 728), (705, 729), (706, 730), (707, 731), (708, 732), (709, 733), (710, 734), (711, 735), (712, 736), (713, 737), (714, 738), (715, 739), (716, 740), (717, 741), (718, 742), (719, 743), (720, 744), (721, 745), (722, 746), (723, 747), (724, 748), (725, 749), (726, 750), (727, 751), (728, 752), (729, 753), (730, 754), (731, 755), (732, 756), (733, 757), (734, 758), (735, 759), (736, 760), (737, 761), (738, 762), (739, 763), (740, 764), (741, 765), (742, 766), (743, 767), (744, 768), (745, 769), (746, 770), (747, 771), (748, 772), (749, 773), (750, 774), (751, 775), (752, 776), (753, 777), (754, 778), (755, 779), (756, 780), (757, 781), (758, 782), (759, 783), (760, 784), (761, 785), (762, 786), (763, 787), (764, 788), (765, 789), (766, 790), (767, 791), (768, 792), (769, 793), (770, 794), (771, 795), (772, 796), (773, 797), (774, 798), (775, 799), (776, 800), (777, 801), (778, 802), (779, 803), (780, 804), (781, 805), (782, 806), (783, 807), (784, 808), (785, 809), (786, 810), (787, 811), (788, 812), (789, 813), (790, 814), (791, 815), (792, 816), (793, 817), (794, 818), (795, 819), (796, 820), (797, 821), (798, 822), (799, 823), (800, 824), (801, 825), (802, 826), (803, 827), (804, 828), (805, 829), (806, 830), (807, 831), (808, 832), (809, 833), (810, 834), (811, 835), (812, 836), (813, 837), (814, 838), (815, 839), (816, 840), (817, 841), (818, 842), (819, 843), (820, 844), (821, 845), (822, 846), (823, 847), (824, 848), (825, 849), (826, 850), (827, 851), (828, 852), (829, 853), (830, 854), (831, 855), (832, 856), (833, 857), (834, 858), (835, 859), (836, 860), (837, 861), (838, 862), (839, 863), (840, 864), (841, 865), (842, 866), (843, 867), (844, 868), (845, 869), (846, 870), (847, 871), (848, 872), (849, 873), (850, 874), (851, 875), (852, 876), (853, 877), (854, 878), (855, 879), (856, 880), (857, 881), (858, 882), (859, 883), (860, 884), (861, 885), (862, 886), (863, 887), (864, 888), (865, 889), (866, 890), (867, 891), (868, 892), (869, 893), (870, 894), (871, 895), (872, 896), (873, 897), (874, 898), (875, 899), (876, 900), (877, 901), (878, 902), (879, 903), (880, 904), (881, 905), (882, 906), (883, 907), (884, 908), (885, 909), (886, 910), (887, 911), (888, 912), (889, 913), (890, 914), (891, 915), (892, 916), (893, 917), (894, 918), (895, 919), (896, 920), (897, 921), (898, 922), (899, 923), (900, 924), (901, 925), (902, 926), (903, 927), (904, 928), (905, 929), (906, 930), (907, 931), (908, 932), (909, 933), (910, 934), (911, 935), (912, 936), (913, 937), (914, 938), (915, 939), (916, 940), (917, 941), (918, 942), (919, 943), (920, 944), (921, 945), (922, 946), (923, 947), (924, 948), (925, 949), (926, 950), (927, 951), (928, 952), (929, 953), (930, 954), (931, 955), (932, 956), (933, 957), (934, 958), (935, 959), (936, 960), (937, 961), (938, 962), (939, 963), (940, 964), (941, 965), (942, 966), (943, 967), (944, 968), (945, 969), (946, 970), (947, 971), (948, 972), (949, 973), (950, 974), (951, 975), (952, 976), (953, 977), (954, 978), (955, 979), (956, 980), (957, 981), (958, 982), (959, 983), (960, 984), (961, 985), (962, 986), (963, 987), (964, 988), (965, 989), (966, 990), (967, 991), (968, 992), (969, 993), (970, 994), (971, 995), (972, 996), (973, 997), (974, 998), (975, 999), (976, 1000), (977, 1001), (978, 1002), (979, 1003), (980, 1004), (981, 1005), (982, 1006), (983, 1007), (984, 1008), (985, 1009), (986, 1010), (987, 1011), (988, 1012), (989, 1013), (990, 1014), (991, 1015), (992, 1016), (993, 1017), (994, 1018), (995, 1019), (996, 1020), (997, 1021), (998, 1022), (999, 1023), (1000, 1024), (1001, 1025), (1002, 1026), (1003, 1027), (1004, 1028), (1005, 1029), (1006, 1030), (1007, 1031), (1008, 1032), (1009, 1033), (1010, 1034), (1011, 1035), (1012, 1036), (1013, 1037), (1014, 1038), (1015, 1039), (1016, 1040), (1017, 1041), (1018, 1042), (1019, 1043), (1020, 1044), (1021, 1045), (1022, 1046), (1023, 1047), (1024, 1048), (1025, 1049), (1026, 1050), (1027, 1051), (1028, 1052), (1029, 1053), (1030, 1054), (1031, 1055), (1032, 1056), (1033, 1057), (1034, 1058), (1035, 1059), (1036, 1060), (1037, 1061), (1038, 1062), (1039, 1063), (1040, 1064), (1041, 1065), (1042, 1066), (1043, 1067), (1044, 1068), (1045, 1069), (1046, 1070), (1047, 1071), (1048, 1072), (1049, 1073), (1050, 1074), (1051, 1075), (1052, 1076), (1053, 1077), (1054, 1078), (1055, 1079), (1056, 1080), (1057, 1081), (1058, 1082), (1059, 1083), (1060, 1084), (1061, 1085), (1062, 1086), (1063, 1087), (1064, 1088), (1065, 1089), (1066, 1090), (1067, 1091), (1068, 1092), (1069, 1093), (1070, 1094), (1071, 1095), (1072, 1096), (1073, 1097), (1074, 1098), (1075, 1099), (1076, 1100), (1077, 1101), (1078, 1102), (1079, 1103), (1080, 1104), (1081, 1105), (1082, 1106), (1083, 1107), (1084, 1108), (1085, 1109), (1086, 1110), (1087, 1111), (1088, 1112), (1089, 1113), (1090, 1114), (1091, 1115), (1092, 1116), (1093, 1117), (1094, 1118), (1095, 1119), (1096, 1120), (1097, 1121), (1098, 1122), (1099, 1123), (1100, 1124), (1101, 1125), (1102, 1126), (1103, 1127), (1104, 1128), (1105, 1129), (1106, 1130), (1107, 1131), (1108, 1132), (1109, 1133), (1110, 1134), (1111, 1135), (1112, 1136), (1113, 1137), (1114, 1138), (1115, 1139), (1116, 1140), (1117, 1141), (1118, 1142), (1119, 1143), (1120, 1144), (1121, 1145), (1122, 1146), (1123, 1147), (1124, 1148), (1125, 1149), (1126, 1150), (1127, 1151), (1128, 1152), (1129, 1153), (1130, 1154), (1131, 1155), (1132, 1156), (1133, 1157), (1134, 1158), (1135, 1159), (1136, 1160), (1137, 1161), (1138, 1162), (1139, 1163), (1140, 1164), (1141, 1165), (1142, 1166), (1143, 1167), (1144, 1168), (1145, 1169), (1146, 1170), (1147, 1171), (1148, 1172), (1149, 1173), (1150, 1174), (1151, 1175), (1152, 1176), (1153, 1177), (1154, 1178), (1155, 1179), (1156, 1180), (1157, 1181), (1158, 1182), (1159, 1183), (1160, 1184), (1161, 1185), (1162, 1186), (1163, 1187), (1164, 1188), (1165, 1189), (1166, 1190), (1167, 1191), (1168, 1192), (1169, 1193), (1170, 1194), (1171, 1195), (1172, 1196), (1173, 1197), (1174, 1198), (1175, 1199), (1176, 1200), (1177, 1201), (1178, 1202), (1179, 1203), (1180, 1204), (1181, 1205), (1182, 1206), (1183, 1207), (1184, 1208), (1185, 1209), (1186, 1210), (1187, 1211), (1188, 1212), (1189, 1213), (1190, 1214), (1191, 1215), (1192, 1216), (1193, 1217), (1194, 1218), (1195, 1219), (1196, 1220), (1197, 1221), (1198, 1222), (1199, 1223), (1200, 1224), (1201, 1225), (1202, 1226), (1203, 1227), (1204, 1228), (1205, 1229), (1206, 1230), (1207, 1231), (1208, 1232), (1209, 1233), (1210, 1234), (1211, 1235), (1212, 1236), (1213, 1237), (1214, 1238), (1215, 1239), (1216, 1240), (1217, 1241), (1218, 1242), (1219, 1243), (1220, 1244), (1221, 1245), (1222, 1246), (1223, 1247), (1224, 1248), (1225, 1249), (1226, 1250), (1227, 1251), (1228, 1252), (1229, 1253), (1230, 1254), (1231, 1255), (1232, 1256), (1233, 1257), (1234, 1258), (1235, 1259), (1236, 1260), (1237, 1261), (1238, 1262), (1239, 1263), (1240, 1264), (1241, 1265), (1242, 1266), (1243, 1267), (1244, 1268), (1245, 1269), (1246, 1270), (1247, 1271), (1248, 1272), (1249, 1273), (1250, 1274), (1251, 1275), (1252, 1276), (1253, 1277), (1254, 1278), (1255, 1279), (1256, 1280), (1257, 1281), (1258, 1282), (1259, 1283), (1260, 1284), (1261, 1285), (1262, 1286), (1263, 1287), (1264, 1288), (1265, 1289), (1266, 1290), (1267, 1291), (1268, 1292), (1269, 1293), (1270, 1294), (1271, 1295), (1272, 1296), (1273, 1297), (1274, 1298), (1275, 1299), (1276, 1300), (1277, 1301), (1278, 1302), (1279, 1303), (1280, 1304), (1281, 1305), (1282, 1306), (1283, 1307), (1284, 1308), (1285, 1309), (1286, 1310), (1287, 1311), (1288, 1312), (1289, 1313), (1290, 1314), (1291, 1315), (1292, 1316), (1293, 1317), (1294, 1318), (1295, 1319), (1296, 1320), (1297, 1321), (1298, 1322), (1299, 1323), (1300, 1324), (1301, 1325), (1302, 1326), (1303, 1327), (1304, 1328), (1305, 1329), (1306, 1330), (1307, 1331), (1308, 1332), (1309, 1333), (1310, 1334), (1311, 1335), (1312, 1336), (1313, 1337), (1314, 1338), (1315, 1339), (1316, 1340), (1317, 1341), (1318, 1342), (1319, 1343), (1320, 1344), (1321, 1345), (1322, 1346), (1323, 1347), (1324, 1348), (1325, 1349), (1326, 1350), (1327, 1351), (1328, 1352), (1329, 1353), (1330, 1354), (1331, 1355), (1332, 1356), (1333, 1357), (1334, 1358), (1335, 1359), (1336, 1360), (1337, 1361), (1338, 1362), (1339, 1363), (1340, 1364), (1341, 1365), (1342, 1366), (1343, 1367), (1344, 1368), (1345, 1369), (1346, 1370), (1347, 1371), (1348, 1372), (1349, 1373), (1350, 1374), (1351, 1375), (1352, 1376), (1353, 1377), (1354, 1378), (1355, 1379), (1356, 1380), (1357, 1381), (1358, 1382), (1359, 1383), (1360, 1384), (1361, 1385), (1362, 1386), (1363, 1387), (1364, 1388), (1365, 1389), (1366, 1390), (1367, 1391), (1368, 1392), (1369, 1393), (1370, 1394), (1371, 1395), (1372, 1396), (1373, 1397), (1374, 1398), (1375, 1399), (1376, 1400), (1377, 1401), (1378, 1402), (1379, 1403), (1380, 1404), (1381, 1405), (1382, 1406), (1383, 1407), (1384, 1408), (1385, 1409), (1386, 1410), (1387, 1411), (1388, 1412), (1389, 1413), (1390, 1414), (1391, 1415), (1392, 1416), (1393, 1417), (1394, 1418), (1395, 1419), (1396, 1420), (1397, 1421), (1398, 1422), (1399, 1423), (1400, 1424), (1401, 1425), (1402, 1426), (1403, 1427), (1404, 1428), (1405, 1429), (1406, 1430), (1407, 1431), (1408, 1432), (1409, 1433), (1410, 1434), (1411, 1435), (1412, 1436), (1413, 1437), (1414, 1438), (1415, 1439), (1416, 1440), (1417, 1441), (1418, 1442), (1419, 1443), (1420, 1444), (1421, 1445), (1422, 1446), (1423, 1447), (1424, 1448), (1425, 1449), (1426, 1450), (1427, 1451), (1428, 1452), (1429, 1453), (1430, 1454), (1431, 1455), (1432, 1456), (1433, 1457), (1434, 1458), (1435, 1459), (1436, 1460), (1437, 1461), (1438, 1462), (1439, 1463), (1440, 1464), (1441, 1465), (1442, 1466), (1443, 1467), (1444, 1468), (1445, 1469), (1446, 1470), (1447, 1471), (1448, 1472), (1449, 1473), (1450, 1474), (1451, 1475), (1452, 1476), (1453, 1477), (1454, 1478), (1455, 1479), (1456, 1480), (1457, 1481), (1458, 1482), (1459, 1483), (1460, 1484), (1461, 1485), (1462, 1486), (1463, 1487), (1464, 1488), (1465, 1489), (1466, 1490), (1467, 1491), (1468, 1492), (1469, 1493), (1470, 1494), (1471, 1495), (1472, 1496), (1473, 1497), (1474, 1498), (1475, 1499), (1476, 1500), (1477, 1501), (1478, 1502), (1479, 1503), (1480, 1504), (1481, 1505), (1482, 1506), (1483, 1507), (1484, 1508), (1485, 1509), (1486, 1510), (1487, 1511), (1488, 1512), (1489, 1513), (1490, 1514), (1491, 1515), (1492, 1516), (1493, 1517), (1494, 1518), (1495, 1519), (1496, 1520), (1497, 1521), (1498, 1522), (1499, 1523), (1500, 1524), (1501, 1525), (1502, 1526), (1503, 1527), (1504, 1528), (1505, 1529), (1506, 1530), (1507, 1531), (1508, 1532), (1509, 1533), (1510, 1534), (1511, 1535), (1512, 1536), (1513, 1537), (1514, 1538), (1515, 1539), (1516, 1540), (1517, 1541), (1518, 1542), (1519, 1543), (1520, 1544), (1521, 1545), (1522, 1546), (1523, 1547), (1524, 1548), (1525, 1549), (1526, 1550), (1527, 1551), (1528, 1552), (1529, 1553), (1530, 1554), (1531, 1555), (1532, 1556), (1533, 1557), (1534, 1558), (1535, 1559), (1536, 1560), (1537, 1561), (1538, 1562), (1539, 1563), (1540, 1564), (1541, 1565), (1542, 1566), (1543, 1567), (1544, 1568), (1545, 1569), (1546, 1570), (1547, 1571), (1548, 1572), (1549, 1573), (1550, 1574), (1551, 1575), (1552, 1576), (1553, 1577), (1554, 1578), (1555, 1579), (1556, 1580), (1557, 1581), (1558, 1582), (1559, 1583), (1560, 1584), (1561, 1585), (1562, 1586), (1563, 1587), (1564, 1588), (1565, 1589), (1566, 1590), (1567, 1591), (1568, 1592), (1569, 1593), (1570, 1594), (1571, 1595), (1572, 1596), (1573, 1597), (1574, 1598), (1575, 1599), (1576, 1600), (1577, 1601), (1578, 1602), (1579, 1603), (1580, 1604), (1581, 1605), (1582, 1606), (1583, 1607), (1584, 1608), (1585, 1609), (1586, 1610), (1587, 1611), (1588, 1612), (1589, 1613), (1590, 1614), (1591, 1615), (1592, 1616), (1593, 1617), (1594, 1618), (1595, 1619), (1596, 1620), (1597, 1621), (1598, 1622), (1599, 1623), (1600, 1624), (1601, 1625), (1602, 1626), (1603, 1627), (1604, 1628), (1605, 1629), (1606, 1630), (1607, 1631), (1608, 1632), (1609, 1633), (1610, 1634), (1611, 1635), (1612, 1636), (1613, 1637), (1614, 1638), (1615, 1639), (1616, 1640), (1617, 1641), (1618, 1642), (1619, 1643), (1620, 1644), (1621, 1645), (1622, 1646), (1623, 1647), (1624, 1648), (1625, 1649), (1626, 1650), (1627, 1651), (1628, 1652), (1629, 1653), (1630, 1654), (1631, 1655), (1632, 1656), (1633, 1657), (1634, 1658), (1635, 1659), (1636, 1660), (1637, 1661), (1638, 1662), (1639, 1663), (1640, 1664), (1641, 1665), (1642, 1666), (1643, 1667), (1644, 1668), (1645, 1669), (1646, 1670), (1647, 1671), (1648, 1672), (1649, 1673), (1650, 1674), (1651, 1675), (1652, 1676), (1653, 1677), (1654, 1678), (1655, 1679), (1656, 1680), (1657, 1681), (1658, 1682), (1659, 1683), (1660, 1684), (1661, 1685), (1662, 1686), (1663, 1687), (1664, 1688), (1665, 1689), (1666, 1690), (1667, 1691), (1668, 1692), (1669, 1693), (1670, 1694), (1671, 1695), (1672, 1696), (1673, 1697), (1674, 1698), (1675, 1699), (1676, 1700), (1677, 1701), (1678, 1702), (1679, 1703), (1680, 1704), (1681, 1705), (1682, 1706), (1683, 1707), (1684, 1708), (1685, 1709), (1686, 1710), (1687, 1711), (1688, 1712), (1689, 1713), (1690, 1714), (1691, 1715), (1692, 1716), (1693, 1717), (1694, 1718), (1695, 1719), (1696, 1720), (1697, 1721), (1698, 1722), (1699, 1723), (1700, 1724), (1701, 1725), (1702, 1726), (1703, 1727), (1704, 1728), (1705, 1729), (1706, 1730), (1707, 1731), (1708, 1732), (1709, 1733), (1710, 1734), (1711, 1735), (1712, 1736), (1713, 1737), (1714, 1738), (1715, 1739), (1716, 1740), (1717, 1741), (1718, 1742), (1719, 1743), (1720, 1744), (1721, 1745), (1722, 1746), (1723, 1747), (1724, 1748), (1725, 1749), (1726, 1750), (1727, 1751), (1728, 1752), (1729, 1753), (1730, 1754), (1731, 1755), (1732, 1756), (1733, 1757), (1734, 1758), (1735, 1759), (1736, 1760), (1737, 1761), (1738, 1762), (1739, 1763), (1740, 1764), (1741, 1765), (1742, 1766), (1743, 1767), (1744, 1768), (1745, 1769), (1746, 1770), (1747, 1771), (1748, 1772), (1749, 1773), (1750, 1774), (1751, 1775), (1752, 1776), (1753, 1777), (1754, 1778), (1755, 1779), (1756, 1780), (1757, 1781), (1758, 1782), (1759, 1783), (1760, 1784), (1761, 1785), (1762, 1786), (1763, 1787), (1764, 1788), (1765, 1789), (1766, 1790), (1767, 1791), (1768, 1792), (1769, 1793), (1770, 1794), (1771, 1795), (1772, 1796), (1773, 1797), (1774, 1798), (1775, 1799), (1776, 1800), (1777, 1801), (1778, 1802), (1779, 1803), (1780, 1804), (1781, 1805), (1782, 1806), (1783, 1807), (1784, 1808), (1785, 1809), (1786, 1810), (1787, 1811), (1788, 1812), (1789, 1813), (1790, 1814), (1791, 1815), (1792, 1816), (1793, 1817), (1794, 1818), (1795, 1819), (1796, 1820), (1797, 1821), (1798, 1822), (1799, 1823), (1800, 1824), (1801, 1825), (1802, 1826), (1803, 1827), (1804, 1828), (1805, 1829), (1806, 1830), (1807, 1831), (1808, 1832), (1809, 1833), (1810, 1834), (1811, 1835), (1812, 1836), (1813, 1837), (1814, 1838), (1815, 1839), (1816, 1840), (1817, 1841), (1818, 1842), (1819, 1843), (1820, 1844), (1821, 1845), (1822, 1846), (1823, 1847), (1824, 1848), (1825, 1849), (1826, 1850), (1827, 1851), (1828, 1852), (1829, 1853), (1830, 1854), (1831, 1855), (1832, 1856), (1833, 1857), (1834, 1858), (1835, 1859), (1836, 1860), (1837, 1861), (1838, 1862), (1839, 1863), (1840, 1864), (1841, 1865), (1842, 1866), (1843, 1867), (1844, 1868), (1845, 1869), (1846, 1870), (1847, 1871), (1848, 1872), (1849, 1873), (1850, 1874), (1851, 1875), (1852, 1876), (1853, 1877), (1854, 1878), (1855, 1879), (1856, 1880), (1857, 1881), (1858, 1882), (1859, 1883), (1860, 1884), (1861, 1885), (1862, 1886), (1863, 1887), (1864, 1888), (1865, 1889), (1866, 1890), (1867, 1891), (1868, 1892), (1869, 1893), (1870, 1894), (1871, 1895), (1872, 1896), (1873, 1897), (1874, 1898), (1875, 1899), (1876, 1900), (1877, 1901), (1878, 1902), (1879, 1903), (1880, 1904), (1881, 1905), (1882, 1906), (1883, 1907), (1884, 1908), (1885, 1909), (1886, 1910), (1887, 1911), (1888, 1912), (1889, 1913), (1890, 1914), (1891, 1915), (1892, 1916), (1893, 1917), (1894, 1918), (1895, 1919), (1896, 1920), (1897, 1921), (1898, 1922), (1899, 1923), (1900, 1924), (1901, 1925), (1902, 1926), (1903, 1927), (1904, 1928), (1905, 1929), (1906, 1930), (1907, 1931), (1908, 1932), (1909, 1933), (1910, 1934), (1911, 1935), (1912, 1936), (1913, 1937), (1914, 1938), (1915, 1939), (1916, 1940), (1917, 1941), (1918, 1942), (1919, 1943), (1920, 1944), (1921, 1945), (1922, 1946), (1923, 1947), (1924, 1948), (1925, 1949), (1926, 1950), (1927, 1951), (1928, 1952), (1929, 1953), (1930, 1954), (1931, 1955), (1932, 1956), (1933, 1957), (1934, 1958), (1935, 1959), (1936, 1960), (1937, 1961), (1938, 1962), (1939, 1963), (1940, 1964), (1941, 1965), (1942, 1966), (1943, 1967), (1944, 1968), (1945, 1969), (1946, 1970), (1947, 1971), (1948, 1972), (1949, 1973), (1950, 1974), (1951, 1975), (1952, 1976), (1953, 1977), (1954, 1978), (1955, 1979), (1956, 1980), (1957, 1981), (1958, 1982), (1959, 1983), (1960, 1984), (1961, 1985), (1962, 1986), (1963, 1987), (1964, 1988), (1965, 1989), (1966, 1990), (1967, 1991), (1968, 1992), (1969, 1993), (1970, 1994), (1971, 1995), (1972, 1996), (1973, 1997), (1974, 1998), (1975, 1999), (1976, 2000), (1977, 2001), (1978, 2002), (1979, 2003), (1980, 2004), (1981, 2005), (1982, 2006), (1983, 2007), (1984, 2008), (1985, 2009), (1986, 2010), (1987, 2011), (1988, 2012), (1989, 2013), (1990, 2014), (1991, 2015), (1992, 2016), (1993, 2017), (1994, 2018), (1995, 2019), (1996, 2020), (1997, 2021), (1998, 2022), (1999, 2023), (2000, 2024), (2001, 2025), (2002, 2026), (2003, 2027), (2004, 2028), (2005, 2029), (2006, 2030), (2007, 2031), (2008, 2032), (2009, 2033), (2010, 2034), (2011, 2035), (2012, 2036), (2013, 2037), (2014, 2038), (2015, 2039), (2016, 2040), (2017, 2041), (2018, 2042), (2019, 2043), (2020, 2044), (2021, 2045), (2022, 2046), (2023, 2047), (2024, 2048), (2025, 2049), (2026, 2050), (2027, 2051), (2028, 2052), (2029, 2053), (2030, 2054), (2031, 2055), (2032, 2056), (2033, 2057), (2034, 2058), (2035, 2059), (2036, 2060), (2037, 2061), (2038, 2062), (2039, 2063), (2040, 2064), (2041, 2065), (2042, 2066), (2043, 2067), (2044, 2068), (2045, 2069), (2046, 2070), (2047, 2071), (2048, 2072), (2049, 2073), (2050, 2074), (2051, 2075), (2052, 2076), (2053, 2077), (2054, 2078), (2055, 2079), (2056, 2080), (2057, 2081), (2058, 2082), (2059, 2083), (2060, 2084), (2061, 2085), (2062, 2086), (2063, 2087), (2064, 2088), (2065, 2089), (2066, 2090), (2067, 2091), (2068, 2092), (2069, 2093), (2070, 2094), (2071, 2095), (2072, 2096), (2073, 2097), (2074, 2098), (2075, 2099), (2076, 2100), (2077, 2101), (2078, 2102), (2079, 2103), (2080, 2104), (2081, 2105), (2082, 2106), (2083, 2107), (2084, 2108), (2085, 2109), (2086, 2110), (2087, 2111), (2088, 2112), (2089, 2113), (2090, 2114), (2091, 2115), (2092, 2116), (2093, 2117), (2094, 2118), (2095, 2119), (2096, 2120), (2097, 2121), (2098, 2122), (2099, 2123), (2100, 2124), (2101, 2125), (2102, 2126), (2103, 2127), (2104, 2128), (2105, 2129), (2106, 2130), (2107, 2131), (2108, 2132), (2109, 2133), (2110, 2134), (2111, 2135), (2112, 2136), (2113, 2137), (2114, 2138), (2115, 2139), (2116, 2140), (2117, 2141), (2118, 2142), (2119, 2143), (2120, 2144), (2121, 2145), (2122, 2146), (2123, 2147), (2124, 2148), (2125, 2149), (2126, 2150), (2127, 2151), (2128, 2152), (2129, 2153), (2130, 2154), (2131, 2155), (2132, 2156), (2133, 2157), (2134, 2158), (2135, 2159), (2136, 2160), (2137, 2161), (2138, 2162), (2139, 2163), (2140, 2164), (2141, 2165), (2142, 2166), (2143, 2167), (2144, 2168), (2145, 2169), (2146, 2170), (2147, 2171), (2148, 2172), (2149, 2173), (2150, 2174), (2151, 2175), (2152, 2176), (2153, 2177), (2154, 2178), (2155, 2179), (2156, 2180), (2157, 2181), (2158, 2182), (2159, 2183), (2160, 2184), (2161, 2185), (2162, 2186), (2163, 2187), (2164, 2188), (2165, 2189), (2166, 2190), (2167, 2191), (2168, 2192), (2169, 2193), (2170, 2194), (2171, 2195), (2172, 2196), (2173, 2197), (2174, 2198), (2175, 2199), (2176, 2200), (2177, 2201), (2178, 2202), (2179, 2203), (2180, 2204), (2181, 2205), (2182, 2206), (2183, 2207), (2184, 2208), (2185, 2209), (2186, 2210), (2187, 2211), (2188, 2212), (2189, 2213), (2190, 2214), (2191, 2215), (2192, 2216), (2193, 2217), (2194, 2218), (2195, 2219), (2196, 2220), (2197, 2221), (2198, 2222), (2199, 2223), (2200, 2224), (2201, 2225), (2202, 2226), (2203, 2227), (2204, 2228), (2205, 2229), (2206, 2230), (2207, 2231), (2208, 2232), (2209, 2233), (2210, 2234), (2211, 2235), (2212, 2236), (2213, 2237), (2214, 2238), (2215, 2239), (2216, 2240), (2217, 2241), (2218, 2242), (2219, 2243), (2220, 2244), (2221, 2245), (2222, 2246), (2223, 2247), (2224, 2248), (2225, 2249), (2226, 2250), (2227, 2251), (2228, 2252), (2229, 2253), (2230, 2254), (2231, 2255), (2232, 2256), (2233, 2257), (2234, 2258), (2235, 2259), (2236, 2260), (2237, 2261), (2238, 2262), (2239, 2263), (2240, 2264), (2241, 2265), (2242, 2266), (2243, 2267), (2244, 2268), (2245, 2269), (2246, 2270), (2247, 2271), (2248, 2272), (2249, 2273), (2250, 2274), (2251, 2275), (2252, 2276), (2253, 2277), (2254, 2278), (2255, 2279), (2256, 2280), (2257, 2281), (2258, 2282), (2259, 2283), (2260, 2284), (2261, 2285), (2262, 2286), (2263, 2287), (2264, 2288), (2265, 2289), (2266, 2290), (2267, 2291), (2268, 2292), (2269, 2293), (2270, 2294), (2271, 2295), (2272, 2296), (2273, 2297), (2274, 2298), (2275, 2299), (2276, 2300), (2277, 2301), (2278, 2302), (2279, 2303), (2280, 2304), (2281, 2305), (2282, 2306), (2283, 2307), (2284, 2308), (2285, 2309), (2286, 2310), (2287, 2311), (2288, 2312), (2289, 2313), (2290, 2314), (2291, 2315), (2292, 2316), (2293, 2317), (2294, 2318), (2295, 2319), (2296, 2320), (2297, 2321), (2298, 2322), (2299, 2323), (2300, 2324), (2301, 2325), (2302, 2326), (2303, 2327), (2304, 2328), (2305, 2329), (2306, 2330), (2307, 2331), (2308, 2332), (2309, 2333), (2310, 2334), (2311, 2335), (2312, 2336), (2313, 2337), (2314, 2338), (2315, 2339), (2316, 2340), (2317, 2341), (2318, 2342), (2319, 2343), (2320, 2344), (2321, 2345), (2322, 2346), (2323, 2347), (2324, 2348), (2325, 2349), (2326, 2350), (2327, 2351), (2328, 2352), (2329, 2353), (2330, 2354), (2331, 2355), (2332, 2356), (2333, 2357), (2334, 2358), (2335, 2359), (2336, 2360), (2337, 2361), (2338, 2362), (2339, 2363), (2340, 2364), (2341, 2365), (2342, 2366), (2343, 2367), (2344, 2368), (2345, 2369), (2346, 2370), (2347, 2371), (2348, 2372), (2349, 2373), (2350, 2374), (2351, 2375), (2352, 2376), (2353, 2377), (2354, 2378), (2355, 2379), (2356, 2380), (2357, 2381), (2358, 2382), (2359, 2383), (2360, 2384), (2361, 2385), (2362, 2386), (2363, 2387), (2364, 2388), (2365, 2389), (2366, 2390), (2367, 2391), (2368, 2392), (2369, 2393), (2370, 2394), (2371, 2395), (2372, 2396), (2373, 2397), (2374, 2398), (2375, 2399), (2376, 2400), (2377, 2401), (2378, 2402), (2379, 2403), (2380, 2404), (2381, 2405), (2382, 2406), (2383, 2407), (2384, 2408), (2385, 2409), (2386, 2410), (2387, 2411), (2388, 2412), (2389, 2413), (2390, 2414), (2391, 2415), (2392, 2416), (2393, 2417), (2394, 2418), (2395, 2419), (2396, 2420), (2397, 2421), (2398, 2422), (2399, 2423), (2400, 2424), (2401, 2425), (2402, 2426), (2403, 2427), (2404, 2428), (2405, 2429), (2406, 2430), (2407, 2431), (2408, 2432), (2409, 2433), (2410, 2434), (2411, 2435), (2412, 2436), (2413, 2437), (2414, 2438), (2415, 2439), (2416, 2440), (2417, 2441), (2418, 2442), (2419, 2443), (2420, 2444), (2421, 2445), (2422, 2446), (2423, 2447), (2424, 2448), (2425, 2449), (2426, 2450), (2427, 2451), (2428, 2452), (2429, 2453), (2430, 2454), (2431, 2455), (2432, 2456), (2433, 2457), (2434, 2458), (2435, 2459), (2436, 2460), (2437, 2461), (2438, 2462), (2439, 2463), (2440, 2464), (2441, 2465), (2442, 2466), (2443, 2467), (2444, 2468), (2445, 2469), (2446, 2470), (2447, 2471), (2448, 2472), (2449, 2473), (2450, 2474), (2451, 2475), (2452, 2476), (2453, 2477), (2454, 2478), (2455, 2479), (2456, 2480), (2457, 2481), (2458, 2482), (2459, 2483), (2460, 2484), (2461, 2485), (2462, 2486), (2463, 2487), (2464, 2488), (2465, 2489), (2466, 2490), (2467, 2491), (2468, 2492), (2469, 2493), (2470, 2494), (2471, 2495), (2472, 2496), (2473, 2497)]\n",
            "Dataset type:   <torch_geometric_temporal.signal.static_graph_temporal_signal_batch.StaticGraphTemporalSignalBatch object at 0x7dd368fbf190>\n"
          ]
        }
      ],
      "source": [
        "loader = MadridDatasetLoader(data_test_norm, edges, edge_weights, batch)\n",
        "dataset_test = loader.get_dataset(num_timesteps_in=12, num_timesteps_out=12)\n",
        "print(\"Dataset type:  \", dataset_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bQ2A9CVG-wn2",
        "outputId": "f1e38d73-b092-4589-8312-b175b61da516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 24), (1, 25), (2, 26), (3, 27), (4, 28), (5, 29), (6, 30), (7, 31), (8, 32), (9, 33), (10, 34), (11, 35), (12, 36), (13, 37), (14, 38), (15, 39), (16, 40), (17, 41), (18, 42), (19, 43), (20, 44), (21, 45), (22, 46), (23, 47), (24, 48), (25, 49), (26, 50), (27, 51), (28, 52), (29, 53), (30, 54), (31, 55), (32, 56), (33, 57), (34, 58), (35, 59), (36, 60), (37, 61), (38, 62), (39, 63), (40, 64), (41, 65), (42, 66), (43, 67), (44, 68), (45, 69), (46, 70), (47, 71), (48, 72), (49, 73), (50, 74), (51, 75), (52, 76), (53, 77), (54, 78), (55, 79), (56, 80), (57, 81), (58, 82), (59, 83), (60, 84), (61, 85), (62, 86), (63, 87), (64, 88), (65, 89), (66, 90), (67, 91), (68, 92), (69, 93), (70, 94), (71, 95), (72, 96), (73, 97), (74, 98), (75, 99), (76, 100), (77, 101), (78, 102), (79, 103), (80, 104), (81, 105), (82, 106), (83, 107), (84, 108), (85, 109), (86, 110), (87, 111), (88, 112), (89, 113), (90, 114), (91, 115), (92, 116), (93, 117), (94, 118), (95, 119), (96, 120), (97, 121), (98, 122), (99, 123), (100, 124), (101, 125), (102, 126), (103, 127), (104, 128), (105, 129), (106, 130), (107, 131), (108, 132), (109, 133), (110, 134), (111, 135), (112, 136), (113, 137), (114, 138), (115, 139), (116, 140), (117, 141), (118, 142), (119, 143), (120, 144), (121, 145), (122, 146), (123, 147), (124, 148), (125, 149), (126, 150), (127, 151), (128, 152), (129, 153), (130, 154), (131, 155), (132, 156), (133, 157), (134, 158), (135, 159), (136, 160), (137, 161), (138, 162), (139, 163), (140, 164), (141, 165), (142, 166), (143, 167), (144, 168), (145, 169), (146, 170), (147, 171), (148, 172), (149, 173), (150, 174), (151, 175), (152, 176), (153, 177), (154, 178), (155, 179), (156, 180), (157, 181), (158, 182), (159, 183), (160, 184), (161, 185), (162, 186), (163, 187), (164, 188), (165, 189), (166, 190), (167, 191), (168, 192), (169, 193), (170, 194), (171, 195), (172, 196), (173, 197), (174, 198), (175, 199), (176, 200), (177, 201), (178, 202), (179, 203), (180, 204), (181, 205), (182, 206), (183, 207), (184, 208), (185, 209), (186, 210), (187, 211), (188, 212), (189, 213), (190, 214), (191, 215), (192, 216), (193, 217), (194, 218), (195, 219), (196, 220), (197, 221), (198, 222), (199, 223), (200, 224), (201, 225), (202, 226), (203, 227), (204, 228), (205, 229), (206, 230), (207, 231), (208, 232), (209, 233), (210, 234), (211, 235), (212, 236), (213, 237), (214, 238), (215, 239), (216, 240), (217, 241), (218, 242), (219, 243), (220, 244), (221, 245), (222, 246), (223, 247), (224, 248), (225, 249), (226, 250), (227, 251), (228, 252), (229, 253), (230, 254), (231, 255), (232, 256), (233, 257), (234, 258), (235, 259), (236, 260), (237, 261), (238, 262), (239, 263), (240, 264), (241, 265), (242, 266), (243, 267), (244, 268), (245, 269), (246, 270), (247, 271), (248, 272), (249, 273), (250, 274), (251, 275), (252, 276), (253, 277), (254, 278), (255, 279), (256, 280), (257, 281), (258, 282), (259, 283), (260, 284), (261, 285), (262, 286), (263, 287), (264, 288), (265, 289), (266, 290), (267, 291), (268, 292), (269, 293), (270, 294), (271, 295), (272, 296), (273, 297), (274, 298), (275, 299), (276, 300), (277, 301), (278, 302), (279, 303), (280, 304), (281, 305), (282, 306), (283, 307), (284, 308), (285, 309), (286, 310), (287, 311), (288, 312), (289, 313), (290, 314), (291, 315), (292, 316), (293, 317), (294, 318), (295, 319), (296, 320), (297, 321), (298, 322), (299, 323), (300, 324), (301, 325), (302, 326), (303, 327), (304, 328), (305, 329), (306, 330), (307, 331), (308, 332), (309, 333), (310, 334), (311, 335), (312, 336), (313, 337), (314, 338), (315, 339), (316, 340), (317, 341), (318, 342), (319, 343), (320, 344), (321, 345), (322, 346), (323, 347), (324, 348), (325, 349), (326, 350), (327, 351), (328, 352), (329, 353), (330, 354), (331, 355), (332, 356), (333, 357), (334, 358), (335, 359), (336, 360), (337, 361), (338, 362), (339, 363), (340, 364), (341, 365), (342, 366), (343, 367), (344, 368), (345, 369), (346, 370), (347, 371), (348, 372), (349, 373), (350, 374), (351, 375), (352, 376), (353, 377), (354, 378), (355, 379), (356, 380), (357, 381), (358, 382), (359, 383), (360, 384), (361, 385), (362, 386), (363, 387), (364, 388), (365, 389), (366, 390), (367, 391), (368, 392), (369, 393), (370, 394), (371, 395), (372, 396), (373, 397), (374, 398), (375, 399), (376, 400), (377, 401), (378, 402), (379, 403), (380, 404), (381, 405), (382, 406), (383, 407), (384, 408), (385, 409), (386, 410), (387, 411), (388, 412), (389, 413), (390, 414), (391, 415), (392, 416), (393, 417), (394, 418), (395, 419), (396, 420), (397, 421), (398, 422), (399, 423), (400, 424), (401, 425), (402, 426), (403, 427), (404, 428), (405, 429), (406, 430), (407, 431), (408, 432), (409, 433), (410, 434), (411, 435), (412, 436), (413, 437), (414, 438), (415, 439), (416, 440), (417, 441), (418, 442), (419, 443), (420, 444), (421, 445), (422, 446), (423, 447), (424, 448), (425, 449), (426, 450), (427, 451), (428, 452), (429, 453), (430, 454), (431, 455), (432, 456), (433, 457), (434, 458), (435, 459), (436, 460), (437, 461), (438, 462), (439, 463), (440, 464), (441, 465), (442, 466), (443, 467), (444, 468), (445, 469), (446, 470), (447, 471), (448, 472), (449, 473), (450, 474), (451, 475), (452, 476), (453, 477), (454, 478), (455, 479), (456, 480), (457, 481), (458, 482), (459, 483), (460, 484), (461, 485), (462, 486), (463, 487), (464, 488), (465, 489), (466, 490), (467, 491), (468, 492), (469, 493), (470, 494), (471, 495), (472, 496), (473, 497), (474, 498), (475, 499), (476, 500), (477, 501), (478, 502), (479, 503), (480, 504), (481, 505), (482, 506), (483, 507), (484, 508), (485, 509), (486, 510), (487, 511), (488, 512), (489, 513), (490, 514), (491, 515), (492, 516), (493, 517), (494, 518), (495, 519), (496, 520), (497, 521), (498, 522), (499, 523), (500, 524), (501, 525), (502, 526), (503, 527), (504, 528), (505, 529), (506, 530), (507, 531), (508, 532), (509, 533), (510, 534), (511, 535), (512, 536), (513, 537), (514, 538), (515, 539), (516, 540), (517, 541), (518, 542), (519, 543), (520, 544), (521, 545), (522, 546), (523, 547), (524, 548), (525, 549), (526, 550), (527, 551), (528, 552), (529, 553), (530, 554), (531, 555), (532, 556), (533, 557), (534, 558), (535, 559), (536, 560), (537, 561), (538, 562), (539, 563), (540, 564), (541, 565), (542, 566), (543, 567), (544, 568), (545, 569), (546, 570), (547, 571), (548, 572), (549, 573), (550, 574), (551, 575), (552, 576), (553, 577), (554, 578), (555, 579), (556, 580), (557, 581), (558, 582), (559, 583), (560, 584), (561, 585), (562, 586), (563, 587), (564, 588), (565, 589), (566, 590), (567, 591), (568, 592), (569, 593), (570, 594), (571, 595), (572, 596), (573, 597), (574, 598), (575, 599), (576, 600), (577, 601), (578, 602), (579, 603), (580, 604), (581, 605), (582, 606), (583, 607), (584, 608), (585, 609), (586, 610), (587, 611), (588, 612), (589, 613), (590, 614), (591, 615), (592, 616), (593, 617), (594, 618), (595, 619), (596, 620), (597, 621), (598, 622), (599, 623), (600, 624), (601, 625), (602, 626), (603, 627), (604, 628), (605, 629), (606, 630), (607, 631), (608, 632), (609, 633), (610, 634), (611, 635), (612, 636), (613, 637), (614, 638), (615, 639), (616, 640), (617, 641), (618, 642), (619, 643), (620, 644), (621, 645), (622, 646), (623, 647), (624, 648), (625, 649), (626, 650), (627, 651), (628, 652), (629, 653), (630, 654), (631, 655), (632, 656), (633, 657), (634, 658), (635, 659), (636, 660), (637, 661), (638, 662), (639, 663), (640, 664), (641, 665), (642, 666), (643, 667), (644, 668), (645, 669), (646, 670), (647, 671), (648, 672), (649, 673), (650, 674), (651, 675), (652, 676), (653, 677), (654, 678), (655, 679), (656, 680), (657, 681), (658, 682), (659, 683), (660, 684), (661, 685), (662, 686), (663, 687), (664, 688), (665, 689), (666, 690), (667, 691), (668, 692), (669, 693), (670, 694), (671, 695), (672, 696), (673, 697), (674, 698), (675, 699), (676, 700), (677, 701), (678, 702), (679, 703), (680, 704), (681, 705), (682, 706), (683, 707), (684, 708), (685, 709), (686, 710), (687, 711), (688, 712), (689, 713), (690, 714), (691, 715), (692, 716), (693, 717), (694, 718), (695, 719), (696, 720), (697, 721), (698, 722), (699, 723), (700, 724), (701, 725), (702, 726), (703, 727), (704, 728), (705, 729), (706, 730), (707, 731), (708, 732), (709, 733), (710, 734), (711, 735), (712, 736), (713, 737), (714, 738), (715, 739), (716, 740), (717, 741), (718, 742), (719, 743), (720, 744), (721, 745), (722, 746), (723, 747), (724, 748), (725, 749), (726, 750), (727, 751), (728, 752), (729, 753), (730, 754), (731, 755), (732, 756), (733, 757), (734, 758), (735, 759), (736, 760), (737, 761), (738, 762), (739, 763), (740, 764), (741, 765), (742, 766), (743, 767), (744, 768), (745, 769), (746, 770), (747, 771), (748, 772), (749, 773), (750, 774), (751, 775), (752, 776), (753, 777), (754, 778), (755, 779), (756, 780), (757, 781), (758, 782), (759, 783), (760, 784), (761, 785), (762, 786), (763, 787), (764, 788), (765, 789), (766, 790), (767, 791), (768, 792), (769, 793), (770, 794), (771, 795), (772, 796), (773, 797), (774, 798), (775, 799), (776, 800), (777, 801), (778, 802), (779, 803), (780, 804), (781, 805), (782, 806), (783, 807), (784, 808), (785, 809), (786, 810), (787, 811), (788, 812), (789, 813), (790, 814), (791, 815), (792, 816), (793, 817), (794, 818), (795, 819), (796, 820), (797, 821), (798, 822), (799, 823), (800, 824), (801, 825), (802, 826), (803, 827), (804, 828), (805, 829), (806, 830), (807, 831), (808, 832), (809, 833), (810, 834), (811, 835), (812, 836), (813, 837), (814, 838), (815, 839), (816, 840), (817, 841), (818, 842), (819, 843), (820, 844), (821, 845), (822, 846), (823, 847), (824, 848), (825, 849), (826, 850), (827, 851), (828, 852), (829, 853), (830, 854), (831, 855), (832, 856), (833, 857), (834, 858), (835, 859), (836, 860), (837, 861), (838, 862), (839, 863), (840, 864), (841, 865), (842, 866), (843, 867), (844, 868), (845, 869), (846, 870), (847, 871), (848, 872), (849, 873), (850, 874), (851, 875), (852, 876), (853, 877), (854, 878), (855, 879), (856, 880), (857, 881), (858, 882), (859, 883), (860, 884), (861, 885), (862, 886), (863, 887), (864, 888), (865, 889), (866, 890), (867, 891), (868, 892), (869, 893), (870, 894), (871, 895), (872, 896), (873, 897), (874, 898), (875, 899), (876, 900), (877, 901), (878, 902), (879, 903), (880, 904), (881, 905), (882, 906), (883, 907), (884, 908), (885, 909), (886, 910), (887, 911), (888, 912), (889, 913), (890, 914), (891, 915), (892, 916), (893, 917), (894, 918), (895, 919), (896, 920), (897, 921), (898, 922), (899, 923), (900, 924), (901, 925), (902, 926), (903, 927), (904, 928), (905, 929), (906, 930), (907, 931), (908, 932), (909, 933), (910, 934), (911, 935), (912, 936), (913, 937), (914, 938), (915, 939), (916, 940), (917, 941), (918, 942), (919, 943), (920, 944), (921, 945), (922, 946), (923, 947), (924, 948), (925, 949), (926, 950), (927, 951), (928, 952), (929, 953), (930, 954), (931, 955), (932, 956), (933, 957), (934, 958), (935, 959), (936, 960), (937, 961), (938, 962), (939, 963), (940, 964), (941, 965), (942, 966), (943, 967), (944, 968), (945, 969), (946, 970), (947, 971), (948, 972), (949, 973), (950, 974), (951, 975), (952, 976), (953, 977), (954, 978), (955, 979), (956, 980), (957, 981), (958, 982), (959, 983), (960, 984), (961, 985), (962, 986), (963, 987), (964, 988), (965, 989), (966, 990), (967, 991), (968, 992), (969, 993), (970, 994), (971, 995), (972, 996), (973, 997), (974, 998), (975, 999), (976, 1000), (977, 1001), (978, 1002), (979, 1003), (980, 1004), (981, 1005), (982, 1006), (983, 1007), (984, 1008), (985, 1009), (986, 1010), (987, 1011), (988, 1012), (989, 1013), (990, 1014), (991, 1015), (992, 1016), (993, 1017), (994, 1018), (995, 1019), (996, 1020), (997, 1021), (998, 1022), (999, 1023), (1000, 1024), (1001, 1025), (1002, 1026), (1003, 1027), (1004, 1028), (1005, 1029), (1006, 1030), (1007, 1031), (1008, 1032), (1009, 1033), (1010, 1034), (1011, 1035), (1012, 1036), (1013, 1037), (1014, 1038), (1015, 1039), (1016, 1040), (1017, 1041), (1018, 1042), (1019, 1043), (1020, 1044), (1021, 1045), (1022, 1046), (1023, 1047), (1024, 1048), (1025, 1049), (1026, 1050), (1027, 1051), (1028, 1052), (1029, 1053), (1030, 1054), (1031, 1055), (1032, 1056), (1033, 1057), (1034, 1058), (1035, 1059), (1036, 1060), (1037, 1061), (1038, 1062), (1039, 1063), (1040, 1064), (1041, 1065), (1042, 1066), (1043, 1067), (1044, 1068), (1045, 1069), (1046, 1070), (1047, 1071), (1048, 1072), (1049, 1073), (1050, 1074), (1051, 1075), (1052, 1076), (1053, 1077), (1054, 1078), (1055, 1079), (1056, 1080), (1057, 1081), (1058, 1082), (1059, 1083), (1060, 1084), (1061, 1085), (1062, 1086), (1063, 1087), (1064, 1088), (1065, 1089), (1066, 1090), (1067, 1091), (1068, 1092), (1069, 1093), (1070, 1094), (1071, 1095), (1072, 1096), (1073, 1097), (1074, 1098), (1075, 1099), (1076, 1100), (1077, 1101), (1078, 1102), (1079, 1103), (1080, 1104), (1081, 1105), (1082, 1106), (1083, 1107), (1084, 1108), (1085, 1109), (1086, 1110), (1087, 1111), (1088, 1112), (1089, 1113), (1090, 1114), (1091, 1115), (1092, 1116), (1093, 1117), (1094, 1118), (1095, 1119), (1096, 1120), (1097, 1121), (1098, 1122), (1099, 1123), (1100, 1124), (1101, 1125), (1102, 1126), (1103, 1127), (1104, 1128), (1105, 1129), (1106, 1130), (1107, 1131), (1108, 1132), (1109, 1133), (1110, 1134), (1111, 1135), (1112, 1136), (1113, 1137), (1114, 1138), (1115, 1139), (1116, 1140), (1117, 1141), (1118, 1142), (1119, 1143), (1120, 1144), (1121, 1145), (1122, 1146), (1123, 1147), (1124, 1148), (1125, 1149), (1126, 1150), (1127, 1151), (1128, 1152), (1129, 1153), (1130, 1154), (1131, 1155), (1132, 1156), (1133, 1157), (1134, 1158), (1135, 1159), (1136, 1160), (1137, 1161), (1138, 1162), (1139, 1163), (1140, 1164), (1141, 1165), (1142, 1166), (1143, 1167), (1144, 1168), (1145, 1169), (1146, 1170), (1147, 1171), (1148, 1172), (1149, 1173), (1150, 1174), (1151, 1175), (1152, 1176), (1153, 1177), (1154, 1178), (1155, 1179), (1156, 1180), (1157, 1181), (1158, 1182), (1159, 1183), (1160, 1184), (1161, 1185), (1162, 1186), (1163, 1187), (1164, 1188), (1165, 1189), (1166, 1190), (1167, 1191), (1168, 1192), (1169, 1193), (1170, 1194), (1171, 1195), (1172, 1196), (1173, 1197), (1174, 1198), (1175, 1199), (1176, 1200), (1177, 1201), (1178, 1202), (1179, 1203), (1180, 1204), (1181, 1205), (1182, 1206), (1183, 1207), (1184, 1208), (1185, 1209), (1186, 1210), (1187, 1211), (1188, 1212), (1189, 1213), (1190, 1214), (1191, 1215), (1192, 1216), (1193, 1217), (1194, 1218), (1195, 1219), (1196, 1220), (1197, 1221), (1198, 1222), (1199, 1223), (1200, 1224), (1201, 1225), (1202, 1226), (1203, 1227), (1204, 1228), (1205, 1229), (1206, 1230), (1207, 1231), (1208, 1232), (1209, 1233), (1210, 1234), (1211, 1235), (1212, 1236), (1213, 1237), (1214, 1238), (1215, 1239), (1216, 1240), (1217, 1241), (1218, 1242), (1219, 1243), (1220, 1244), (1221, 1245), (1222, 1246), (1223, 1247), (1224, 1248), (1225, 1249), (1226, 1250), (1227, 1251), (1228, 1252), (1229, 1253), (1230, 1254), (1231, 1255), (1232, 1256), (1233, 1257), (1234, 1258), (1235, 1259), (1236, 1260), (1237, 1261), (1238, 1262), (1239, 1263), (1240, 1264), (1241, 1265), (1242, 1266), (1243, 1267), (1244, 1268), (1245, 1269), (1246, 1270), (1247, 1271), (1248, 1272), (1249, 1273), (1250, 1274), (1251, 1275), (1252, 1276), (1253, 1277), (1254, 1278), (1255, 1279), (1256, 1280), (1257, 1281), (1258, 1282), (1259, 1283), (1260, 1284), (1261, 1285), (1262, 1286), (1263, 1287), (1264, 1288), (1265, 1289), (1266, 1290), (1267, 1291), (1268, 1292), (1269, 1293), (1270, 1294), (1271, 1295), (1272, 1296), (1273, 1297), (1274, 1298), (1275, 1299), (1276, 1300), (1277, 1301), (1278, 1302), (1279, 1303), (1280, 1304), (1281, 1305), (1282, 1306), (1283, 1307), (1284, 1308), (1285, 1309), (1286, 1310), (1287, 1311), (1288, 1312), (1289, 1313), (1290, 1314), (1291, 1315), (1292, 1316), (1293, 1317), (1294, 1318), (1295, 1319), (1296, 1320), (1297, 1321), (1298, 1322), (1299, 1323), (1300, 1324), (1301, 1325), (1302, 1326), (1303, 1327), (1304, 1328), (1305, 1329), (1306, 1330), (1307, 1331), (1308, 1332), (1309, 1333), (1310, 1334), (1311, 1335), (1312, 1336), (1313, 1337), (1314, 1338), (1315, 1339), (1316, 1340), (1317, 1341), (1318, 1342), (1319, 1343), (1320, 1344), (1321, 1345), (1322, 1346), (1323, 1347), (1324, 1348), (1325, 1349), (1326, 1350), (1327, 1351), (1328, 1352), (1329, 1353), (1330, 1354), (1331, 1355), (1332, 1356), (1333, 1357), (1334, 1358), (1335, 1359), (1336, 1360), (1337, 1361), (1338, 1362), (1339, 1363), (1340, 1364), (1341, 1365), (1342, 1366), (1343, 1367), (1344, 1368), (1345, 1369), (1346, 1370), (1347, 1371), (1348, 1372), (1349, 1373), (1350, 1374), (1351, 1375), (1352, 1376), (1353, 1377), (1354, 1378), (1355, 1379), (1356, 1380), (1357, 1381), (1358, 1382), (1359, 1383), (1360, 1384), (1361, 1385), (1362, 1386), (1363, 1387), (1364, 1388), (1365, 1389), (1366, 1390), (1367, 1391), (1368, 1392), (1369, 1393), (1370, 1394), (1371, 1395), (1372, 1396), (1373, 1397), (1374, 1398), (1375, 1399), (1376, 1400), (1377, 1401), (1378, 1402), (1379, 1403), (1380, 1404), (1381, 1405), (1382, 1406), (1383, 1407), (1384, 1408), (1385, 1409), (1386, 1410), (1387, 1411), (1388, 1412), (1389, 1413), (1390, 1414), (1391, 1415), (1392, 1416), (1393, 1417), (1394, 1418), (1395, 1419), (1396, 1420), (1397, 1421), (1398, 1422), (1399, 1423), (1400, 1424), (1401, 1425), (1402, 1426), (1403, 1427), (1404, 1428), (1405, 1429), (1406, 1430), (1407, 1431), (1408, 1432), (1409, 1433), (1410, 1434), (1411, 1435), (1412, 1436), (1413, 1437), (1414, 1438), (1415, 1439), (1416, 1440), (1417, 1441), (1418, 1442), (1419, 1443), (1420, 1444), (1421, 1445), (1422, 1446), (1423, 1447), (1424, 1448), (1425, 1449), (1426, 1450), (1427, 1451), (1428, 1452), (1429, 1453), (1430, 1454), (1431, 1455), (1432, 1456), (1433, 1457), (1434, 1458), (1435, 1459), (1436, 1460), (1437, 1461), (1438, 1462), (1439, 1463), (1440, 1464), (1441, 1465), (1442, 1466), (1443, 1467), (1444, 1468), (1445, 1469), (1446, 1470), (1447, 1471), (1448, 1472), (1449, 1473), (1450, 1474), (1451, 1475), (1452, 1476), (1453, 1477), (1454, 1478), (1455, 1479), (1456, 1480), (1457, 1481), (1458, 1482), (1459, 1483), (1460, 1484), (1461, 1485), (1462, 1486), (1463, 1487), (1464, 1488), (1465, 1489), (1466, 1490), (1467, 1491), (1468, 1492), (1469, 1493), (1470, 1494), (1471, 1495), (1472, 1496), (1473, 1497), (1474, 1498), (1475, 1499), (1476, 1500), (1477, 1501), (1478, 1502), (1479, 1503), (1480, 1504), (1481, 1505), (1482, 1506), (1483, 1507), (1484, 1508), (1485, 1509), (1486, 1510), (1487, 1511), (1488, 1512), (1489, 1513), (1490, 1514), (1491, 1515), (1492, 1516), (1493, 1517), (1494, 1518), (1495, 1519), (1496, 1520), (1497, 1521), (1498, 1522), (1499, 1523), (1500, 1524), (1501, 1525), (1502, 1526), (1503, 1527), (1504, 1528), (1505, 1529), (1506, 1530), (1507, 1531), (1508, 1532), (1509, 1533), (1510, 1534), (1511, 1535), (1512, 1536), (1513, 1537), (1514, 1538), (1515, 1539), (1516, 1540), (1517, 1541), (1518, 1542), (1519, 1543), (1520, 1544), (1521, 1545), (1522, 1546), (1523, 1547), (1524, 1548), (1525, 1549), (1526, 1550), (1527, 1551), (1528, 1552), (1529, 1553), (1530, 1554), (1531, 1555), (1532, 1556), (1533, 1557), (1534, 1558), (1535, 1559), (1536, 1560), (1537, 1561), (1538, 1562), (1539, 1563), (1540, 1564), (1541, 1565), (1542, 1566), (1543, 1567), (1544, 1568), (1545, 1569), (1546, 1570), (1547, 1571), (1548, 1572), (1549, 1573), (1550, 1574), (1551, 1575), (1552, 1576), (1553, 1577), (1554, 1578), (1555, 1579), (1556, 1580), (1557, 1581), (1558, 1582), (1559, 1583), (1560, 1584), (1561, 1585), (1562, 1586), (1563, 1587), (1564, 1588), (1565, 1589), (1566, 1590), (1567, 1591), (1568, 1592), (1569, 1593), (1570, 1594), (1571, 1595), (1572, 1596), (1573, 1597), (1574, 1598), (1575, 1599), (1576, 1600), (1577, 1601), (1578, 1602), (1579, 1603), (1580, 1604), (1581, 1605), (1582, 1606), (1583, 1607), (1584, 1608), (1585, 1609), (1586, 1610), (1587, 1611), (1588, 1612), (1589, 1613), (1590, 1614), (1591, 1615), (1592, 1616), (1593, 1617), (1594, 1618), (1595, 1619), (1596, 1620), (1597, 1621), (1598, 1622), (1599, 1623), (1600, 1624), (1601, 1625), (1602, 1626), (1603, 1627), (1604, 1628), (1605, 1629), (1606, 1630), (1607, 1631), (1608, 1632), (1609, 1633), (1610, 1634), (1611, 1635), (1612, 1636), (1613, 1637), (1614, 1638), (1615, 1639), (1616, 1640), (1617, 1641), (1618, 1642), (1619, 1643), (1620, 1644), (1621, 1645), (1622, 1646), (1623, 1647), (1624, 1648), (1625, 1649), (1626, 1650), (1627, 1651), (1628, 1652), (1629, 1653), (1630, 1654), (1631, 1655), (1632, 1656), (1633, 1657), (1634, 1658), (1635, 1659), (1636, 1660), (1637, 1661), (1638, 1662), (1639, 1663), (1640, 1664), (1641, 1665), (1642, 1666), (1643, 1667), (1644, 1668), (1645, 1669), (1646, 1670), (1647, 1671), (1648, 1672), (1649, 1673), (1650, 1674), (1651, 1675), (1652, 1676), (1653, 1677), (1654, 1678), (1655, 1679), (1656, 1680), (1657, 1681), (1658, 1682), (1659, 1683), (1660, 1684), (1661, 1685), (1662, 1686), (1663, 1687), (1664, 1688), (1665, 1689), (1666, 1690), (1667, 1691), (1668, 1692), (1669, 1693), (1670, 1694), (1671, 1695), (1672, 1696), (1673, 1697), (1674, 1698), (1675, 1699), (1676, 1700), (1677, 1701), (1678, 1702), (1679, 1703), (1680, 1704), (1681, 1705), (1682, 1706), (1683, 1707), (1684, 1708), (1685, 1709), (1686, 1710), (1687, 1711), (1688, 1712), (1689, 1713), (1690, 1714), (1691, 1715), (1692, 1716), (1693, 1717), (1694, 1718), (1695, 1719), (1696, 1720), (1697, 1721), (1698, 1722), (1699, 1723), (1700, 1724), (1701, 1725), (1702, 1726), (1703, 1727), (1704, 1728), (1705, 1729), (1706, 1730), (1707, 1731), (1708, 1732), (1709, 1733), (1710, 1734), (1711, 1735), (1712, 1736), (1713, 1737), (1714, 1738), (1715, 1739), (1716, 1740), (1717, 1741), (1718, 1742), (1719, 1743), (1720, 1744), (1721, 1745), (1722, 1746), (1723, 1747), (1724, 1748), (1725, 1749), (1726, 1750), (1727, 1751), (1728, 1752), (1729, 1753), (1730, 1754), (1731, 1755), (1732, 1756), (1733, 1757), (1734, 1758), (1735, 1759), (1736, 1760), (1737, 1761), (1738, 1762), (1739, 1763), (1740, 1764), (1741, 1765), (1742, 1766), (1743, 1767), (1744, 1768), (1745, 1769), (1746, 1770), (1747, 1771), (1748, 1772), (1749, 1773), (1750, 1774), (1751, 1775), (1752, 1776), (1753, 1777), (1754, 1778), (1755, 1779), (1756, 1780), (1757, 1781), (1758, 1782), (1759, 1783), (1760, 1784), (1761, 1785), (1762, 1786), (1763, 1787), (1764, 1788), (1765, 1789), (1766, 1790), (1767, 1791), (1768, 1792), (1769, 1793), (1770, 1794), (1771, 1795), (1772, 1796), (1773, 1797), (1774, 1798), (1775, 1799), (1776, 1800), (1777, 1801), (1778, 1802), (1779, 1803), (1780, 1804), (1781, 1805), (1782, 1806), (1783, 1807), (1784, 1808), (1785, 1809), (1786, 1810), (1787, 1811), (1788, 1812), (1789, 1813), (1790, 1814), (1791, 1815), (1792, 1816), (1793, 1817), (1794, 1818), (1795, 1819), (1796, 1820), (1797, 1821), (1798, 1822), (1799, 1823), (1800, 1824), (1801, 1825), (1802, 1826), (1803, 1827), (1804, 1828), (1805, 1829), (1806, 1830), (1807, 1831), (1808, 1832), (1809, 1833), (1810, 1834), (1811, 1835), (1812, 1836), (1813, 1837), (1814, 1838), (1815, 1839), (1816, 1840), (1817, 1841), (1818, 1842), (1819, 1843), (1820, 1844), (1821, 1845), (1822, 1846), (1823, 1847), (1824, 1848), (1825, 1849), (1826, 1850), (1827, 1851), (1828, 1852), (1829, 1853), (1830, 1854), (1831, 1855), (1832, 1856), (1833, 1857), (1834, 1858), (1835, 1859), (1836, 1860), (1837, 1861), (1838, 1862), (1839, 1863), (1840, 1864), (1841, 1865), (1842, 1866), (1843, 1867), (1844, 1868), (1845, 1869), (1846, 1870), (1847, 1871), (1848, 1872), (1849, 1873), (1850, 1874), (1851, 1875), (1852, 1876), (1853, 1877), (1854, 1878), (1855, 1879), (1856, 1880), (1857, 1881), (1858, 1882), (1859, 1883), (1860, 1884), (1861, 1885), (1862, 1886), (1863, 1887), (1864, 1888), (1865, 1889), (1866, 1890), (1867, 1891), (1868, 1892), (1869, 1893), (1870, 1894), (1871, 1895), (1872, 1896), (1873, 1897), (1874, 1898), (1875, 1899), (1876, 1900), (1877, 1901), (1878, 1902), (1879, 1903), (1880, 1904), (1881, 1905), (1882, 1906), (1883, 1907), (1884, 1908), (1885, 1909), (1886, 1910), (1887, 1911), (1888, 1912), (1889, 1913), (1890, 1914), (1891, 1915), (1892, 1916), (1893, 1917), (1894, 1918), (1895, 1919), (1896, 1920), (1897, 1921), (1898, 1922), (1899, 1923), (1900, 1924), (1901, 1925), (1902, 1926), (1903, 1927), (1904, 1928), (1905, 1929), (1906, 1930), (1907, 1931), (1908, 1932), (1909, 1933), (1910, 1934), (1911, 1935), (1912, 1936), (1913, 1937), (1914, 1938), (1915, 1939), (1916, 1940), (1917, 1941), (1918, 1942), (1919, 1943), (1920, 1944), (1921, 1945), (1922, 1946), (1923, 1947), (1924, 1948), (1925, 1949), (1926, 1950), (1927, 1951), (1928, 1952), (1929, 1953), (1930, 1954), (1931, 1955), (1932, 1956), (1933, 1957), (1934, 1958), (1935, 1959), (1936, 1960), (1937, 1961), (1938, 1962), (1939, 1963), (1940, 1964), (1941, 1965), (1942, 1966), (1943, 1967), (1944, 1968), (1945, 1969), (1946, 1970), (1947, 1971), (1948, 1972), (1949, 1973), (1950, 1974), (1951, 1975), (1952, 1976), (1953, 1977), (1954, 1978), (1955, 1979), (1956, 1980), (1957, 1981), (1958, 1982), (1959, 1983), (1960, 1984), (1961, 1985), (1962, 1986), (1963, 1987), (1964, 1988), (1965, 1989), (1966, 1990), (1967, 1991), (1968, 1992), (1969, 1993), (1970, 1994), (1971, 1995), (1972, 1996), (1973, 1997), (1974, 1998), (1975, 1999), (1976, 2000), (1977, 2001), (1978, 2002), (1979, 2003), (1980, 2004), (1981, 2005), (1982, 2006), (1983, 2007), (1984, 2008), (1985, 2009), (1986, 2010), (1987, 2011), (1988, 2012), (1989, 2013), (1990, 2014), (1991, 2015), (1992, 2016), (1993, 2017), (1994, 2018), (1995, 2019), (1996, 2020), (1997, 2021), (1998, 2022), (1999, 2023), (2000, 2024), (2001, 2025), (2002, 2026), (2003, 2027), (2004, 2028), (2005, 2029), (2006, 2030), (2007, 2031), (2008, 2032), (2009, 2033), (2010, 2034), (2011, 2035), (2012, 2036), (2013, 2037), (2014, 2038), (2015, 2039), (2016, 2040), (2017, 2041), (2018, 2042), (2019, 2043), (2020, 2044), (2021, 2045), (2022, 2046), (2023, 2047), (2024, 2048), (2025, 2049), (2026, 2050), (2027, 2051), (2028, 2052), (2029, 2053), (2030, 2054), (2031, 2055), (2032, 2056), (2033, 2057), (2034, 2058), (2035, 2059), (2036, 2060), (2037, 2061), (2038, 2062), (2039, 2063), (2040, 2064), (2041, 2065), (2042, 2066), (2043, 2067), (2044, 2068), (2045, 2069), (2046, 2070), (2047, 2071), (2048, 2072), (2049, 2073), (2050, 2074), (2051, 2075), (2052, 2076), (2053, 2077), (2054, 2078), (2055, 2079), (2056, 2080), (2057, 2081), (2058, 2082), (2059, 2083), (2060, 2084), (2061, 2085), (2062, 2086), (2063, 2087), (2064, 2088), (2065, 2089), (2066, 2090), (2067, 2091), (2068, 2092), (2069, 2093), (2070, 2094), (2071, 2095), (2072, 2096), (2073, 2097), (2074, 2098), (2075, 2099), (2076, 2100), (2077, 2101), (2078, 2102), (2079, 2103), (2080, 2104), (2081, 2105), (2082, 2106), (2083, 2107), (2084, 2108), (2085, 2109), (2086, 2110), (2087, 2111), (2088, 2112), (2089, 2113), (2090, 2114), (2091, 2115), (2092, 2116), (2093, 2117), (2094, 2118), (2095, 2119), (2096, 2120), (2097, 2121), (2098, 2122), (2099, 2123), (2100, 2124), (2101, 2125), (2102, 2126), (2103, 2127), (2104, 2128), (2105, 2129), (2106, 2130), (2107, 2131), (2108, 2132), (2109, 2133), (2110, 2134), (2111, 2135), (2112, 2136), (2113, 2137), (2114, 2138), (2115, 2139), (2116, 2140), (2117, 2141), (2118, 2142), (2119, 2143), (2120, 2144), (2121, 2145), (2122, 2146), (2123, 2147), (2124, 2148), (2125, 2149), (2126, 2150), (2127, 2151), (2128, 2152), (2129, 2153), (2130, 2154), (2131, 2155), (2132, 2156), (2133, 2157), (2134, 2158), (2135, 2159), (2136, 2160), (2137, 2161), (2138, 2162), (2139, 2163), (2140, 2164), (2141, 2165), (2142, 2166), (2143, 2167), (2144, 2168), (2145, 2169), (2146, 2170), (2147, 2171), (2148, 2172), (2149, 2173), (2150, 2174), (2151, 2175), (2152, 2176), (2153, 2177), (2154, 2178), (2155, 2179), (2156, 2180), (2157, 2181), (2158, 2182), (2159, 2183), (2160, 2184), (2161, 2185), (2162, 2186), (2163, 2187), (2164, 2188), (2165, 2189), (2166, 2190), (2167, 2191), (2168, 2192), (2169, 2193), (2170, 2194), (2171, 2195), (2172, 2196), (2173, 2197), (2174, 2198), (2175, 2199), (2176, 2200), (2177, 2201), (2178, 2202), (2179, 2203), (2180, 2204), (2181, 2205), (2182, 2206), (2183, 2207), (2184, 2208), (2185, 2209), (2186, 2210), (2187, 2211), (2188, 2212), (2189, 2213), (2190, 2214), (2191, 2215), (2192, 2216), (2193, 2217), (2194, 2218), (2195, 2219), (2196, 2220), (2197, 2221), (2198, 2222), (2199, 2223), (2200, 2224), (2201, 2225), (2202, 2226), (2203, 2227), (2204, 2228), (2205, 2229), (2206, 2230), (2207, 2231), (2208, 2232), (2209, 2233), (2210, 2234), (2211, 2235), (2212, 2236), (2213, 2237), (2214, 2238), (2215, 2239), (2216, 2240), (2217, 2241), (2218, 2242), (2219, 2243), (2220, 2244), (2221, 2245), (2222, 2246), (2223, 2247), (2224, 2248), (2225, 2249), (2226, 2250), (2227, 2251), (2228, 2252), (2229, 2253), (2230, 2254), (2231, 2255), (2232, 2256), (2233, 2257), (2234, 2258), (2235, 2259), (2236, 2260), (2237, 2261), (2238, 2262), (2239, 2263), (2240, 2264), (2241, 2265), (2242, 2266), (2243, 2267), (2244, 2268), (2245, 2269), (2246, 2270), (2247, 2271), (2248, 2272), (2249, 2273), (2250, 2274), (2251, 2275), (2252, 2276), (2253, 2277), (2254, 2278), (2255, 2279), (2256, 2280), (2257, 2281), (2258, 2282), (2259, 2283), (2260, 2284), (2261, 2285), (2262, 2286), (2263, 2287), (2264, 2288), (2265, 2289), (2266, 2290), (2267, 2291), (2268, 2292), (2269, 2293), (2270, 2294), (2271, 2295), (2272, 2296), (2273, 2297), (2274, 2298), (2275, 2299), (2276, 2300), (2277, 2301), (2278, 2302), (2279, 2303), (2280, 2304), (2281, 2305), (2282, 2306), (2283, 2307), (2284, 2308), (2285, 2309), (2286, 2310), (2287, 2311), (2288, 2312), (2289, 2313), (2290, 2314), (2291, 2315), (2292, 2316), (2293, 2317), (2294, 2318), (2295, 2319), (2296, 2320), (2297, 2321), (2298, 2322), (2299, 2323), (2300, 2324), (2301, 2325), (2302, 2326), (2303, 2327), (2304, 2328), (2305, 2329), (2306, 2330), (2307, 2331), (2308, 2332), (2309, 2333), (2310, 2334), (2311, 2335), (2312, 2336), (2313, 2337), (2314, 2338), (2315, 2339), (2316, 2340), (2317, 2341), (2318, 2342), (2319, 2343), (2320, 2344), (2321, 2345), (2322, 2346), (2323, 2347), (2324, 2348), (2325, 2349), (2326, 2350), (2327, 2351), (2328, 2352), (2329, 2353), (2330, 2354), (2331, 2355), (2332, 2356), (2333, 2357), (2334, 2358), (2335, 2359), (2336, 2360), (2337, 2361), (2338, 2362), (2339, 2363), (2340, 2364), (2341, 2365), (2342, 2366), (2343, 2367), (2344, 2368), (2345, 2369), (2346, 2370), (2347, 2371), (2348, 2372), (2349, 2373), (2350, 2374), (2351, 2375), (2352, 2376), (2353, 2377), (2354, 2378), (2355, 2379), (2356, 2380), (2357, 2381), (2358, 2382), (2359, 2383), (2360, 2384), (2361, 2385), (2362, 2386), (2363, 2387), (2364, 2388), (2365, 2389), (2366, 2390), (2367, 2391), (2368, 2392), (2369, 2393), (2370, 2394), (2371, 2395), (2372, 2396), (2373, 2397), (2374, 2398), (2375, 2399), (2376, 2400), (2377, 2401), (2378, 2402), (2379, 2403), (2380, 2404), (2381, 2405), (2382, 2406), (2383, 2407), (2384, 2408), (2385, 2409), (2386, 2410), (2387, 2411), (2388, 2412), (2389, 2413), (2390, 2414), (2391, 2415), (2392, 2416), (2393, 2417), (2394, 2418), (2395, 2419), (2396, 2420), (2397, 2421), (2398, 2422), (2399, 2423), (2400, 2424), (2401, 2425), (2402, 2426), (2403, 2427), (2404, 2428), (2405, 2429), (2406, 2430), (2407, 2431), (2408, 2432), (2409, 2433), (2410, 2434), (2411, 2435), (2412, 2436), (2413, 2437), (2414, 2438), (2415, 2439), (2416, 2440), (2417, 2441), (2418, 2442), (2419, 2443), (2420, 2444), (2421, 2445), (2422, 2446), (2423, 2447), (2424, 2448), (2425, 2449), (2426, 2450), (2427, 2451), (2428, 2452), (2429, 2453), (2430, 2454), (2431, 2455), (2432, 2456), (2433, 2457), (2434, 2458), (2435, 2459), (2436, 2460), (2437, 2461), (2438, 2462), (2439, 2463), (2440, 2464), (2441, 2465), (2442, 2466), (2443, 2467), (2444, 2468), (2445, 2469), (2446, 2470), (2447, 2471), (2448, 2472), (2449, 2473), (2450, 2474), (2451, 2475), (2452, 2476)]\n",
            "Dataset type:   <torch_geometric_temporal.signal.static_graph_temporal_signal_batch.StaticGraphTemporalSignalBatch object at 0x7dd368891090>\n"
          ]
        }
      ],
      "source": [
        "loader = MadridDatasetLoader(data_norm, edges, edge_weights, batch)\n",
        "dataset = loader.get_dataset(num_timesteps_in=12, num_timesteps_out=12)\n",
        "print(\"Dataset type:  \", dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSQER-GswxbR",
        "outputId": "ef1b34dc-3dd6-4004-965e-c36ddc7af4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set as 11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TemporalGNN(\n",
              "  (tgnn): A3TGCN(\n",
              "    (_base_tgcn): TGCN(\n",
              "      (conv_z): GCNConv(18, 256)\n",
              "      (linear_z): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (conv_r): GCNConv(18, 256)\n",
              "      (linear_r): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (conv_h): GCNConv(18, 256)\n",
              "      (linear_h): Linear(in_features=512, out_features=256, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=256, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# define A3T-GCN\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  set_seed(rnd_seed)\n",
        "\n",
        "  unit_num  = 256  # This is number of units in order to construct the architecture of the A3T-GCN.\n",
        "\n",
        "  class TemporalGNN(torch.nn.Module):\n",
        "      def __init__(self, node_features, periods):\n",
        "          super(TemporalGNN, self).__init__()\n",
        "          # Attention Temporal Graph Convolutional Cell\n",
        "          self.tgnn = A3TGCN(in_channels=node_features,\n",
        "                            out_channels=unit_num,\n",
        "                            periods=periods)\n",
        "          # Equals single-shot prediction\n",
        "          self.linear = torch.nn.Linear(unit_num, periods)\n",
        "\n",
        "      def forward(self, x, edge_index,  edge_weight):\n",
        "          \"\"\"\n",
        "          x = Node features for T time steps\n",
        "          edge_index = Graph edge indices\n",
        "          \"\"\"\n",
        "          h = self.tgnn(x, edge_index,  edge_weight)\n",
        "          h = F.relu(h)\n",
        "          h = self.linear(h)\n",
        "          return h\n",
        "\n",
        "TemporalGNN(node_features=18, periods=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fNSDo75_FHfT",
        "outputId": "a3d1b4ad-f1f7-4d12-8bdf-8cf730fe095f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b9bb8b5a13a7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create model and optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTemporalGNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m             )\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cuda_getDeviceCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             raise AssertionError(\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "# GPU support\n",
        "device = torch.device('cuda') # cuda\n",
        "\n",
        "\n",
        "# Create model and optimizers\n",
        "model = TemporalGNN(node_features=18, periods=12).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "model.train()\n",
        "\n",
        "print(\"Running training...\")\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    step = 0\n",
        "    for snapshot in dataset:\n",
        "        snapshot = snapshot.to(device)\n",
        "        # Get model predictions\n",
        "        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
        "        # Mean Squared Error\n",
        "        loss = loss + torch.mean((y_hat-snapshot.y)**2)\n",
        "        step += 1\n",
        "\n",
        "\n",
        "    loss = loss / (step + 1)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(\"Epoch {} train MSE: {:.4f}\".format(epoch, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ZZBeg07mZoXh",
        "outputId": "82914158-d532-4374-8215-bbed9db081ac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c3f3994547c4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Store for analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "loss = 0\n",
        "step = 0\n",
        "\n",
        "# Store for analysis\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "for snapshot in dataset_test:\n",
        "    snapshot = snapshot.to(device)\n",
        "    # Get predictions\n",
        "    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
        "    yhat_reverse = reverse_zscore(y_hat, means[0], stds[0])\n",
        "    snapshot.y_reverse = reverse_zscore(snapshot.y, means[0], stds[0])\n",
        "    # Root Mean Squared Error\n",
        "    loss = loss + torch.sqrt(torch.mean((yhat_reverse-snapshot.y_reverse)**2))\n",
        "    # Store for analysis below\n",
        "    labels.append(snapshot.y_reverse)\n",
        "    predictions.append(yhat_reverse)\n",
        "    step += 1\n",
        "\n",
        "loss = loss / (step+1)\n",
        "loss = loss.item()\n",
        "print(\"Test RMSE: {:.4f}\".format(loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hNERp-_xs27y"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "loss = 0\n",
        "step = 0\n",
        "\n",
        "# Store for analysis\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "for snapshot in dataset_test:\n",
        "    snapshot = snapshot.to(device)\n",
        "    # Get predictions\n",
        "    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
        "    yhat_reverse = reverse_zscore(y_hat, means[0], stds[0])\n",
        "    snapshot.y_reverse = reverse_zscore(snapshot.y, means[0], stds[0])\n",
        "    # Mean Absolute Error\n",
        "    loss = loss + torch.mean(torch.abs(yhat_reverse-snapshot.y_reverse))\n",
        "    # Store for analysis below\n",
        "    labels.append(snapshot.y_reverse)\n",
        "    predictions.append(yhat_reverse)\n",
        "    step += 1\n",
        "\n",
        "\n",
        "loss = loss / (step+1)\n",
        "loss = loss.item()\n",
        "print(\"Test MAE: {:.4f}\".format(loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqmZM5qH_HgJ"
      },
      "outputs": [],
      "source": [
        "# it is calculated for all nodes\n",
        "\n",
        "ALLNode_pred = []\n",
        "ALLNode_true = []\n",
        "for item in predictions:\n",
        "  for node in range(24):\n",
        "    for hour in range(12):\n",
        "      ALLNode_pred.append(item[node][hour].detach().cpu().numpy().item(0))\n",
        "\n",
        "\n",
        "for item in labels:\n",
        "  for node in range(24):\n",
        "    for hour in range(12):\n",
        "      ALLNode_true.append(item[node][hour].detach().cpu().numpy().item(0))\n",
        "\n",
        "\n",
        "ALLNode_pred_np = np.array(ALLNode_pred)\n",
        "ALLNode_pred_np_resh = ALLNode_pred_np.reshape(-1, 24, 12)\n",
        "ALLNode_true_np = np.array(ALLNode_true)\n",
        "ALLNode_true_np_resh = ALLNode_true_np.reshape(-1, 24, 12)\n",
        "\n",
        "\n",
        "scipy.stats.pearsonr(ALLNode_pred_np, ALLNode_true_np)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQhp1HxH5Ljm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vHjh3qQg5LmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a6TMMKbprlc"
      },
      "source": [
        "TGCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAlD0FbX9sIi"
      },
      "outputs": [],
      "source": [
        "#to convert adjacency dataframe to numpy\n",
        "adj = adj_mat_complete.to_numpy()\n",
        "\n",
        "# standardise train data\n",
        "\n",
        "data = fin_data.transpose(\n",
        "            (1, 2, 0)\n",
        "        )\n",
        "data = data.astype(np.float32)\n",
        "\n",
        "# standardise (via Z-Score Method)\n",
        "means = np.mean(data, axis=(0, 2))\n",
        "data_norm= data-means.reshape(1, -1, 1)\n",
        "stds = np.std(data_norm, axis=(0, 2))\n",
        "data_norm= data_norm/ stds.reshape(1, -1, 1)\n",
        "\n",
        "#to convert adjacency matrix and standardised train data to torch\n",
        "adj = torch.from_numpy(adj)\n",
        "data_norm= torch.from_numpy(data_norm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eA65rwi9vpH"
      },
      "outputs": [],
      "source": [
        "# standardise test data using means and standard deviation of train set\n",
        "\n",
        "data_test = fin_data_test.transpose(\n",
        "            (1, 2, 0)\n",
        "        )\n",
        "data_test = data_test.astype(np.float32)\n",
        "data_test_norm= data_test- means.reshape(1, -1, 1)\n",
        "data_test_norm= data_test_norm/ stds.reshape(1, -1, 1)\n",
        "\n",
        "#to convert standardised test data to torch\n",
        "data_test_norm = torch.from_numpy(data_test_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS3BS3j4p-GT",
        "outputId": "78f079be-16f7-49c0-b03a-bfd2be7d91c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([24, 24])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adj.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2kiy4ndqBlM",
        "outputId": "59ceca56-7b29-4b27-a675-5a2e1609aeba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([24, 18, 4343])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_test_norm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vh8TX71qFHL"
      },
      "outputs": [],
      "source": [
        "#from adjacency matrix extract the edge indices and edge weights\n",
        "\n",
        "edge_indices, values = dense_to_sparse(adj)\n",
        "edge_indices = edge_indices.numpy()\n",
        "values = values.numpy()\n",
        "edges = edge_indices\n",
        "edge_weights = values\n",
        "batch =64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjOsJKf0qLoM",
        "outputId": "59c019f2-8345-4d1e-f161-dc95f4d7464d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 552)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edges.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDg-uTUxqPWd",
        "outputId": "99762227-c252-475b-d73c-ec4144725d53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(552,)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edge_weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3NBsG1sqSgP"
      },
      "outputs": [],
      "source": [
        "class MadridDatasetLoader(object):\n",
        "    \"\"\"The dataset is based on 24 stations (nodes) each having 18 features (nodal features)\n",
        "    and 276 edges connecting each pair of nodes, the edge weights are the distance between the edges.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_norm, edges, edge_weights, batch):\n",
        "        super(MadridDatasetLoader, self).__init__()\n",
        "\n",
        "        self.data_norm = data_norm\n",
        "        self.edges = edges\n",
        "        self.edge_weights= edge_weights\n",
        "        self.batch = batch\n",
        "\n",
        "\n",
        "    def _generate_task(self, num_timesteps_in: int = 6, num_timesteps_out: int = 6):\n",
        "        \"\"\"Uses the node features of the graph and generates a feature/target\n",
        "        relationship of the shape\n",
        "        (num_nodes, num_node_features, num_timesteps_in) -> (num_nodes, num_timesteps_out)\n",
        "\n",
        "\n",
        "        Args:\n",
        "            num_timesteps_in (int): number of timesteps the sequence model sees\n",
        "            num_timesteps_out (int): number of timesteps the sequence model has to predict\n",
        "        \"\"\"\n",
        "        time_steps_starter =   36 # it can be assigned as one of the following {0, 12, 24, 36}\n",
        "        indices = [\n",
        "            (i, i +time_steps_starter+ (num_timesteps_in + num_timesteps_out))\n",
        "            for i in range(self.data_norm.shape[2] - (time_steps_starter+num_timesteps_in + num_timesteps_out) + 1)\n",
        "        ]\n",
        "        print(indices)\n",
        "        # Generate observations\n",
        "        features, target = [], []\n",
        "        for i, j in indices:\n",
        "            features.append((self.data_norm[:, :, i : i + num_timesteps_in]).numpy().transpose(\n",
        "            (2, 0, 1)\n",
        "        ))\n",
        "            target.append((self.data_norm[  :, 0, i + num_timesteps_in +time_steps_starter: j]).numpy())\n",
        "\n",
        "        self.features = features\n",
        "        self.targets = target\n",
        "\n",
        "    def get_dataset(\n",
        "        self, num_timesteps_in: int = 6, num_timesteps_out: int = 6\n",
        "    ) -> StaticGraphTemporalSignal:\n",
        "        \"\"\"Returns data iterator for the dataset as an instance of the\n",
        "        static graph temporal signal class.\n",
        "\n",
        "        Return types:\n",
        "            * **dataset** *(StaticGraphTemporalSignal)* - The forecasting dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        self._generate_task(num_timesteps_in, num_timesteps_out)\n",
        "        dataset = StaticGraphTemporalSignalBatch(\n",
        "            self.edges, self.edge_weights, self.features, self.targets, self.batch\n",
        "        )\n",
        "\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr8XG4l0qWv0"
      },
      "outputs": [],
      "source": [
        "loader = MadridDatasetLoader(data_test_norm, edges, edge_weights, batch)\n",
        "dataset_test = loader.get_dataset(num_timesteps_in=12, num_timesteps_out=12)\n",
        "print(\"Dataset type:  \", dataset_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtnqZzRGqbIG"
      },
      "outputs": [],
      "source": [
        "loader = MadridDatasetLoader(data_norm, edges, edge_weights, batch)\n",
        "dataset = loader.get_dataset(num_timesteps_in=12, num_timesteps_out=12)\n",
        "print(\"Dataset type:  \", dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWufZl52s3ev"
      },
      "outputs": [],
      "source": [
        "next(iter(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTPLYwWrrAm7",
        "outputId": "0966c1c4-5212-427e-ebd8-482930a9dda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed set as 11\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TemporalGNN(\n",
              "  (tgnn): TGCN2(\n",
              "    (conv_z): GCNConv(18, 256)\n",
              "    (linear_z): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (conv_r): GCNConv(18, 256)\n",
              "    (linear_r): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (conv_h): GCNConv(18, 256)\n",
              "    (linear_h): Linear(in_features=512, out_features=256, bias=True)\n",
              "  )\n",
              "  (linear): Linear(in_features=256, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define A3T-GCN\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  set_seed(rnd_seed)\n",
        "\n",
        "  unit_num  = 256 # This is number of units in order to construct the architecture of the TGCN.\n",
        "  class TemporalGNN(torch.nn.Module):\n",
        "      def __init__(self, node_features):\n",
        "          super(TemporalGNN, self).__init__()\n",
        "          # Temporal Graph Convolutional Cell\n",
        "          self.tgnn = TGCN2(in_channels=node_features,\n",
        "                            out_channels=unit_num, batch_size=12)\n",
        "          # Equals single-shot prediction\n",
        "          self.linear = torch.nn.Linear(unit_num, 12)\n",
        "\n",
        "      def forward(self, x, edge_index,  edge_weight):\n",
        "          \"\"\"\n",
        "          x = Node features for T time steps\n",
        "          edge_index = Graph edge indices\n",
        "          \"\"\"\n",
        "          h = self.tgnn(x, edge_index,  edge_weight)\n",
        "          h = F.relu(h)\n",
        "          h = self.linear(h)\n",
        "          return h\n",
        "\n",
        "TemporalGNN(node_features=18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS0ChRHVrEom",
        "outputId": "16c36497-cce0-427e-a0b5-82ea8d8081da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running training...\n",
            "Epoch 0 train MSE: 0.9955\n"
          ]
        }
      ],
      "source": [
        "# GPU support\n",
        "device = torch.device('cpu') # cuda\n",
        "\n",
        "\n",
        "# Create model and optimizers\n",
        "model = TemporalGNN(node_features=18).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "model.train()\n",
        "\n",
        "print(\"Running training...\")\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    step = 0\n",
        "    for snapshot in dataset:\n",
        "        snapshot = snapshot.to(device)\n",
        "        # Get model predictions\n",
        "        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
        "        # Mean Squared Error\n",
        "        loss = loss + torch.mean((y_hat-snapshot.y)**2)\n",
        "        step += 1\n",
        "\n",
        "\n",
        "    loss = loss / (step + 1)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(\"Epoch {} train MSE: {:.4f}\".format(epoch, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A2aYqACF1DE"
      },
      "outputs": [],
      "source": [
        "# it is calculated for all nodes\n",
        "\n",
        "ALLNode_pred = []\n",
        "ALLNode_true = []\n",
        "for item in predictions:\n",
        "  for batch in range(12):\n",
        "    for node in range(24):\n",
        "      for hour in range(12):\n",
        "        ALLNode_pred.append(item[batch][node][hour].detach().cpu().numpy().item(0))\n",
        "\n",
        "\n",
        "for item in labels:\n",
        "  for node in range(24):\n",
        "    for hour in range(12):\n",
        "      ALLNode_true.append(item[node][hour].detach().cpu().numpy().item(0))\n",
        "\n",
        "\n",
        "ALLNode_pred_np = np.array(ALLNode_pred)\n",
        "ALLNode_pred_np_resh = ALLNode_pred_np.reshape(-1, 12, 24, 12)\n",
        "ALLNode_true_np = np.array(ALLNode_true)\n",
        "ALLNode_true_np_resh = ALLNode_true_np.reshape(-1, 24, 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cYmTVsDarIYl"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "loss = 0\n",
        "step = 0\n",
        "\n",
        "# Store for analysis\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "for snapshot in dataset_test:\n",
        "    snapshot = snapshot.to(device)\n",
        "    # Get predictions\n",
        "    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
        "    yhat_reverse = reverse_zscore(y_hat, means[0], stds[0])\n",
        "    snapshot.y_reverse = reverse_zscore(snapshot.y, means[0], stds[0])\n",
        "    # Mean Absolute Error\n",
        "    loss = loss + torch.mean(torch.abs(yhat_reverse-snapshot.y_reverse))\n",
        "    # Store for analysis below\n",
        "    labels.append(snapshot.y_reverse)\n",
        "    predictions.append(yhat_reverse)\n",
        "    step += 1\n",
        "\n",
        "\n",
        "loss = loss / (step+1)\n",
        "loss = loss.item()\n",
        "print(\"Test MAE: {:.4f}\".format(loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IU-4otmCF3H"
      },
      "outputs": [],
      "source": [
        "ALLNode_pred_np_resh_Update = np.mean(ALLNode_pred_np_resh, axis=1)\n",
        "true = ALLNode_true_np_resh.reshape(-1)\n",
        "pred = ALLNode_pred_np_resh_Update.reshape(-1)\n",
        "rmse = mean_squared_error(true, pred, squared=False)\n",
        "mae = mean_absolute_error(true, pred)\n",
        "scipy.stats.pearsonr(true, pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SXdB6rvn9wS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yrQ7jEjE5Lr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIylBL5Dujrm"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCB5u6Ei1V46"
      },
      "outputs": [],
      "source": [
        "# standardise train data\n",
        "\n",
        "data = fin_data.transpose(\n",
        "            (1, 2, 0)\n",
        "        )\n",
        "data = data.astype(np.float32)\n",
        "\n",
        "# standardise (via Z-Score Method)\n",
        "means = np.mean(data, axis=(0, 2))\n",
        "data_norm= data-means.reshape(1, -1, 1)\n",
        "stds = np.std(data_norm, axis=(0, 2))\n",
        "data_norm= data_norm/ stds.reshape(1, -1, 1)\n",
        "fin_data_norm = data_norm.transpose(2, 0, 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RaX5h41QUvk",
        "outputId": "6d0106aa-0095-4063-c769-0d523c8e2562"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4344, 24, 18)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fin_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XblUgo_GTldw"
      },
      "outputs": [],
      "source": [
        "# standardise test data\n",
        "\n",
        "data_test = fin_data_test.transpose(\n",
        "            (1, 2, 0)\n",
        "        )\n",
        "data_test = data_test.astype(np.float32)\n",
        "data_test_norm= data_test- means.reshape(1, -1, 1)\n",
        "data_test_norm= data_test_norm/ stds.reshape(1, -1, 1)\n",
        "fin_data_test_norm = data_test_norm.transpose(2, 0, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-GPFJVGTdia",
        "outputId": "bcf49d24-d7c2-4064-cda7-46475e917d44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4343, 24, 18)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fin_data_test_norm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyrT376CXEE1"
      },
      "outputs": [],
      "source": [
        "# split dataset to X and y (dependent and independent)\n",
        "\n",
        "def split_sequence(sequence, seq_notNorm, time_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i +12\n",
        "\t\ttime_steps_starter = 0 # it can be assigned as one of the following {0, 12, 24, 36}\n",
        "\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix+time_steps_starter+time_steps > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], seq_notNorm[end_ix+time_steps_starter:end_ix+time_steps_starter+time_steps]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\n",
        "\treturn np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "# choose a number of time steps\n",
        "time_steps = 12\n",
        "X_train, y_train = split_sequence(fin_data_norm, fin_data_norm, time_steps)\n",
        "X_test, y_test = split_sequence(fin_data_test_norm, fin_data_test_norm, time_steps)\n",
        "\n",
        "# to select only nitrogen dioxide as a target feature\n",
        "y_train = y_train[:, :, :, 0]\n",
        "y_test = y_test[:, :, :, 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TijYCg8j3_49",
        "outputId": "421850c3-94c9-4eb1-cad8-693fb7b5e663"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4319, 12, 24)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9sVQ3kJXpS9",
        "outputId": "8d938829-8fe3-47cd-882c-1372aa7adb16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4320, 12, 24, 18)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te68AEaDjgf2"
      },
      "outputs": [],
      "source": [
        "# reshape the data for LSTM input\n",
        "\n",
        "number_selected_columns =18\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 24*number_selected_columns))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 24*number_selected_columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVf6syYcafEb",
        "outputId": "bae0f38b-8e21-4b11-8dfa-6d7f269b3e30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed set as 11\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  set_seed(rnd_seed)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(432,input_shape=(X_train.shape[1], 24*number_selected_columns)))\n",
        "  model.add(LSTM(512, return_sequences=True))\n",
        "  model.add(Dense(24))\n",
        "  model.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXuHvfdJbKdj"
      },
      "outputs": [],
      "source": [
        "# run LSTM\n",
        "\n",
        "lstm_model = model.fit(X_train_reshaped, y_train, epochs=100, verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzu0SlJezwvO",
        "outputId": "41b86766-6afe-45b8-fd35-da7edb588387"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4319, 12, 24)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0AZoRmFkT5z",
        "outputId": "c09956ff-911c-4218-a32c-b98b622a0678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "135/135 [==============================] - 2s 7ms/step\n"
          ]
        }
      ],
      "source": [
        "yhat = model.predict(X_test_reshaped, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j5AVX_IvXmy"
      },
      "outputs": [],
      "source": [
        "yhat_reverse = reverse_zscore(yhat, means[0], stds[0])\n",
        "ytest_reverse = reverse_zscore(y_test, means[0], stds[0])\n",
        "yhat_reshaped = yhat_reverse.reshape(-1,24)\n",
        "y_test_reshaped= ytest_reverse.reshape(-1,24)\n",
        "rmse = mean_squared_error(yhat_reshaped, y_test_reshaped, squared=False)\n",
        "mae = mean_absolute_error(yhat_reshaped, y_test_reshaped)\n",
        "print('Test Score: %.2f RMSE' % (rmse))\n",
        "print('Test Score: %.2f MAE' % (mae))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHawxiO1vZ-d"
      },
      "outputs": [],
      "source": [
        "scipy.stats.pearsonr(yhat_reshaped.reshape(-1), y_test_reshaped.reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLmjUdCbKXuT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz-AhsM0tkmD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU"
      ],
      "metadata": {
        "id": "dy-VBluzB7Vw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXBobptkCIg4"
      },
      "outputs": [],
      "source": [
        "# standardise train data\n",
        "\n",
        "data = fin_data.transpose(\n",
        "            (1, 2, 0)\n",
        "        )\n",
        "data = data.astype(np.float32)\n",
        "\n",
        "# standardise (via Z-Score Method)\n",
        "means = np.mean(data, axis=(0, 2))\n",
        "data_norm= data-means.reshape(1, -1, 1)\n",
        "stds = np.std(data_norm, axis=(0, 2))\n",
        "data_norm= data_norm/ stds.reshape(1, -1, 1)\n",
        "fin_data_norm = data_norm.transpose(2, 0, 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0106aa-0095-4063-c769-0d523c8e2562",
        "id": "uGSpkn83CMWc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4344, 24, 18)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fin_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U7HiQAwCPmK"
      },
      "outputs": [],
      "source": [
        "# standardise test data\n",
        "\n",
        "data_test = fin_data_test.transpose(\n",
        "            (1, 2, 0)\n",
        "        )\n",
        "data_test = data_test.astype(np.float32)\n",
        "data_test_norm= data_test- means.reshape(1, -1, 1)\n",
        "data_test_norm= data_test_norm/ stds.reshape(1, -1, 1)\n",
        "fin_data_test_norm = data_test_norm.transpose(2, 0, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf49d24-d7c2-4064-cda7-46475e917d44",
        "id": "C65QpL6-CSFe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4343, 24, 18)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fin_data_test_norm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtsDvv5wCU_0"
      },
      "outputs": [],
      "source": [
        "# split dataset to X and y (dependent and independent)\n",
        "\n",
        "def split_sequence(sequence, seq_notNorm, time_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i +12\n",
        "\t\ttime_steps_starter = 0 # it can be assigned as one of the following {0, 12, 24, 36}\n",
        "\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix+time_steps_starter+time_steps > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], seq_notNorm[end_ix+time_steps_starter:end_ix+time_steps_starter+time_steps]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\n",
        "\treturn np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "# choose a number of time steps\n",
        "time_steps = 12\n",
        "X_train, y_train = split_sequence(fin_data_norm, fin_data_norm, time_steps)\n",
        "X_test, y_test = split_sequence(fin_data_test_norm, fin_data_test_norm, time_steps)\n",
        "\n",
        "# to select only nitrogen dioxide as a target feature\n",
        "y_train = y_train[:, :, :, 0]\n",
        "y_test = y_test[:, :, :, 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421850c3-94c9-4eb1-cad8-693fb7b5e663",
        "id": "o0gHvoXiCZ01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4319, 12, 24)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d938829-8fe3-47cd-882c-1372aa7adb16",
        "id": "UXhCwIyrCc6T"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4320, 12, 24, 18)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8_9VpbSutmO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# reshape the data for GRU input\n",
        "\n",
        "number_selected_columns =18\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 24*number_selected_columns))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 24*number_selected_columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A_lSpItuvuf",
        "outputId": "c76d28c7-881e-44ce-b22e-c8e3e9ff85e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed set as 11\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  set_seed(rnd_seed)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(432,input_shape=(X_train.shape[1], 24*number_selected_columns)))\n",
        "  model.add(GRU(512, return_sequences=True))\n",
        "  model.add(Dense(24))\n",
        "  model.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j1_ZirvzuxTG"
      },
      "outputs": [],
      "source": [
        "# run GRU\n",
        "\n",
        "gru_model = model.fit(X_train_reshaped, y_train, epochs=100, verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIFDmFmJndfx",
        "outputId": "6ecc23c0-aae5-4098-f043-7902e6bbd7fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "135/135 [==============================] - 4s 26ms/step\n"
          ]
        }
      ],
      "source": [
        "yhat = model.predict(X_test_reshaped, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q5kp-PwwnhA3"
      },
      "outputs": [],
      "source": [
        "yhat_reverse = reverse_zscore(yhat, means[0], stds[0])\n",
        "ytest_reverse = reverse_zscore(y_test, means[0], stds[0])\n",
        "yhat_reshaped = yhat_reverse.reshape(-1,24)\n",
        "y_test_reshaped= ytest_reverse.reshape(-1,24)\n",
        "rmse = mean_squared_error(yhat_reshaped, y_test_reshaped, squared=False)\n",
        "mae = mean_absolute_error(yhat_reshaped, y_test_reshaped)\n",
        "print('Test Score: %.2f RMSE' % (rmse))\n",
        "print('Test Score: %.2f MAE' % (mae))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JTCzNkMb-37H"
      },
      "outputs": [],
      "source": [
        "scipy.stats.pearsonr(yhat_reshaped.reshape(-1), y_test_reshaped.reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOk9DhIaC5an"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}